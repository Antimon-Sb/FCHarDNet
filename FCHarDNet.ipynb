{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antimon-Sb/FCHarDNet/blob/main/FCHarDNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7aHBeOc7na"
      },
      "outputs": [],
      "source": [
        "!rm LabeledApproved_full.zip\n",
        "!rm 701_StillsRaw_full.zip\n",
        "!wget http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/LabeledApproved_full.zip\n",
        "!wget http://web4.cs.ucl.ac.uk/staff/g.brostow/MotionSegRecData/files/701_StillsRaw_full.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/labeled\n",
        "!mkdir -p data/raw\n",
        "#!unzip LabeledApproved_full.zip -d /content/data/labeled\n",
        "#!unzip /content/701_StillsRaw_full.zip -d /content/data/raw\n",
        "!unzip drive/MyDrive/LabeledApproved_full.zip -d /content/data/labeled\n",
        "!unzip drive/MyDrive/701_StillsRaw_full.zip -d /content/data/raw"
      ],
      "metadata": {
        "id": "GIV5iaazu-17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aiq0BQqu14JY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imageL = plt.imread(\"/content/data/labeled/0001TP_006690_L.png\")\n",
        "image = plt.imread(\"/content/data/raw/701_StillsRaw_full/0001TP_006690.png\")\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.imshow(imageL)\n",
        "plt.subplot(212)\n",
        "plt.imshow(image)\n",
        "print(image.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing data\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "labeled_root = 'data/labeled'\n",
        "raw_root = 'data/raw/701_StillsRaw_full'\n",
        "#print(os.listdir(labeled_root))\n",
        "for file in os.listdir(labeled_root):\n",
        "    if file.find('_L') != -1:\n",
        "      new_file = file[:file.find('_L')] + file[file.find('L') + 1:]\n",
        "    #print(file)\n",
        "      os.rename(labeled_root + '/' + file, labeled_root + '/' + new_file)\n",
        "data_files = os.listdir(raw_root)\n",
        "np.random.shuffle(data_files)\n",
        "os.makedirs('data/test', exist_ok=True)\n",
        "os.makedirs('data/train', exist_ok=True)\n",
        "os.makedirs('data/val', exist_ok=True)\n",
        "os.makedirs('data/test_L', exist_ok=True)\n",
        "os.makedirs('data/train_L', exist_ok=True)\n",
        "os.makedirs('data/val_L', exist_ok=True)\n",
        "os.system('rm data/train/*')\n",
        "os.system('rm data/test/*')\n",
        "os.system('rm data/val/*')\n",
        "train_filenames, testval = np.split(np.array(data_files), [int(len(data_files) * 0.8)])\n",
        "test_filenames, val_filenames = np.split(np.array(testval), [int(len(testval) * 0.5)])\n",
        "\n",
        "train_filenames_L = ['data/labeled/'+ name for name in train_filenames.tolist()]\n",
        "train_filenames = ['data/raw/701_StillsRaw_full/'+ name for name in train_filenames.tolist()]\n",
        "test_filenames_L = ['data/labeled/'+ name for name in test_filenames.tolist()]\n",
        "test_filenames = ['data/raw/701_StillsRaw_full/' + name for name in test_filenames.tolist()]\n",
        "val_filenames_L = ['data/labeled/'+ name for name in val_filenames.tolist()]\n",
        "val_filenames = ['data/raw/701_StillsRaw_full/' + name for name in val_filenames.tolist()]\n",
        "\n",
        "for filename in train_filenames:\n",
        "    shutil.copy(filename, 'data/train')\n",
        "for filename in test_filenames:\n",
        "    shutil.copy(filename, 'data/test')\n",
        "for filename in val_filenames:\n",
        "    shutil.copy(filename, 'data/val')\n",
        "for filename in train_filenames_L:\n",
        "    shutil.copy(filename, 'data/train_L')\n",
        "for filename in test_filenames_L:\n",
        "    shutil.copy(filename, 'data/test_L')\n",
        "for filename in val_filenames_L:\n",
        "    shutil.copy(filename, 'data/val_L')\n",
        "#print(len(os.listdir('data/train')), len(os.listdir('data/test')), len(os.listdir('data/val')))"
      ],
      "metadata": {
        "id": "_ZJUX0u0vF8N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numbers\n",
        "import random\n",
        "import torchvision.transforms.functional as tf\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, augmentations):\n",
        "        self.augmentations = augmentations\n",
        "        self.PIL2Numpy = False\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = Image.fromarray(img, mode=\"RGB\")\n",
        "            mask = Image.fromarray(mask, mode=\"RGB\")\n",
        "            self.PIL2Numpy = True\n",
        "\n",
        "        assert img.size == mask.size\n",
        "        for a in self.augmentations:\n",
        "            img, mask = a(img, mask)\n",
        "\n",
        "        if self.PIL2Numpy:\n",
        "            img, mask = np.array(img), np.array(mask, dtype=np.uint8)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "class RandomHorizontallyFlip(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        if random.random() < self.p:\n",
        "            return (img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "        return img, mask\n",
        "\n",
        "class RandomRotate(object):\n",
        "    def __init__(self, degree):\n",
        "        self.degree = degree\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        rotate_degree = random.random() * 2 * self.degree - self.degree\n",
        "        return (\n",
        "            tf.affine(\n",
        "                img,\n",
        "                translate=(0, 0),\n",
        "                scale=1.0,\n",
        "                angle=rotate_degree,\n",
        "                resample=Image.BILINEAR,\n",
        "                fill=(0, 0, 0),\n",
        "                shear=0.0,\n",
        "            ),\n",
        "            tf.affine(\n",
        "                mask,\n",
        "                translate=(0, 0),\n",
        "                scale=1.0,\n",
        "                angle=rotate_degree,\n",
        "                resample=Image.NEAREST,\n",
        "                fill=250,\n",
        "                shear=0.0,\n",
        "            ),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "vnnm5PF7ply9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import Long\n",
        "#data loader\n",
        "import collections\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.transform\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "def rgb_to_lbl(rgb):\n",
        "    Sky = [128, 128, 128]\n",
        "    Building = [128, 0, 0]\n",
        "    Pole = [192, 192, 128]\n",
        "    Road = [128, 64, 128]\n",
        "    Pavement = [60, 40, 222]\n",
        "    Tree = [128, 128, 0]\n",
        "    SignSymbol = [192, 128, 128]\n",
        "    Fence = [64, 64, 128]\n",
        "    Car = [64, 0, 128]\n",
        "    Pedestrian = [64, 64, 0]\n",
        "    Bicyclist = [0, 128, 192]\n",
        "    Unlabelled = [0, 0, 0]\n",
        "\n",
        "    label_colours = np.array(\n",
        "        [\n",
        "            Sky,\n",
        "            Building,\n",
        "            Pole,\n",
        "            Road,\n",
        "            Pavement,\n",
        "            Tree,\n",
        "            SignSymbol,\n",
        "            Fence,\n",
        "            Car,\n",
        "            Pedestrian,\n",
        "            Bicyclist,\n",
        "            Unlabelled,\n",
        "        ]\n",
        "    )\n",
        "    label = 11 * torch.ones(720 * 960, dtype=torch.uint8)\n",
        "    w, h, t = rgb.size()\n",
        "    rgb2 = rgb[:]\n",
        "    rgb = rgb.view(-1, t)\n",
        "    for l in range(0, len(label_colours)):\n",
        "      r = np.abs(rgb[:, 0]) == label_colours[l, 0]\n",
        "      g = np.abs(rgb[:, 1]) == label_colours[l, 1]\n",
        "      b = np.abs(rgb[:, 2]) == label_colours[l, 2]\n",
        "      tf_rgb = r * g * b\n",
        "      label[tf_rgb] = l\n",
        "    label = label.reshape(720, 960)\n",
        "    #print(rgb2[360][260:270], label[360][260:270])\n",
        "    return label\n",
        "\n",
        "class camvidLoader(data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        split=\"train\",\n",
        "        is_transform=False,\n",
        "        img_size=None,\n",
        "        augmentations=None,\n",
        "        img_norm=True,\n",
        "        test_mode=False,\n",
        "    ):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.img_size = [720, 960]\n",
        "        self.is_transform = is_transform\n",
        "        self.augmentations = augmentations\n",
        "        self.img_norm = img_norm\n",
        "        self.test_mode = test_mode\n",
        "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
        "        self.n_classes = 12\n",
        "        self.files = collections.defaultdict(list)\n",
        "\n",
        "        if not self.test_mode:\n",
        "            for split in [\"train\", \"test\", \"val\"]:\n",
        "                file_list = os.listdir(root + \"/\" + split)\n",
        "                self.files[split] = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.files[self.split][index]\n",
        "        img_path = self.root + \"/\" + self.split + \"/\" + img_name\n",
        "        lbl_path = self.root + \"/\" + self.split + \"_L/\" + img_name\n",
        "\n",
        "        img = plt.imread(img_path)\n",
        "        img = np.array(img*255, dtype=np.uint8)\n",
        "\n",
        "        lbl = plt.imread(lbl_path)\n",
        "        lbl = np.array(lbl*255, dtype=np.int8)\n",
        "\n",
        "        if self.augmentations is not None:\n",
        "            img, lbl = self.augmentations(img, lbl)\n",
        "\n",
        "        if self.is_transform:\n",
        "            img, lbl = self.transform(img, lbl)\n",
        "        return img, lbl\n",
        "\n",
        "    def transform(self, img, lbl):\n",
        "        img = skimage.transform.resize(img, (self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
        "        img = img * 255\n",
        "        img = img[:, :, ::-1]  # RGB -> BGR\n",
        "        img = img.astype(np.float64)\n",
        "        img -= self.mean\n",
        "        if self.img_norm:\n",
        "            # Resize scales images from 0 to 255, thus we need\n",
        "            # to divide by 255.0\n",
        "            img = img.astype(float) / 255.0\n",
        "        # NHWC -> NCHW\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        img = torch.from_numpy(img).float()\n",
        "        lbl = torch.from_numpy(lbl).long()\n",
        "        return img, lbl\n",
        "\n",
        "    def decode_segmap(self, temp, plot=False):\n",
        "        Sky = [128, 128, 128]\n",
        "        Building = [128, 0, 0]\n",
        "        Pole = [192, 192, 128]\n",
        "        Road = [128, 64, 128]\n",
        "        Pavement = [60, 40, 222]\n",
        "        Tree = [128, 128, 0]\n",
        "        SignSymbol = [192, 128, 128]\n",
        "        Fence = [64, 64, 128]\n",
        "        Car = [64, 0, 128]\n",
        "        Pedestrian = [64, 64, 0]\n",
        "        Bicyclist = [0, 128, 192]\n",
        "        Unlabelled = [0, 0, 0]\n",
        "\n",
        "        label_colours = np.array(\n",
        "            [\n",
        "                Sky,\n",
        "                Building,\n",
        "                Pole,\n",
        "                Road,\n",
        "                Pavement,\n",
        "                Tree,\n",
        "                SignSymbol,\n",
        "                Fence,\n",
        "                Car,\n",
        "                Pedestrian,\n",
        "                Bicyclist,\n",
        "                Unlabelled,\n",
        "            ]\n",
        "        )\n",
        "        r = temp.copy()\n",
        "        g = temp.copy()\n",
        "        b = temp.copy()\n",
        "        for l in range(0, self.n_classes):\n",
        "            r[temp == l] = label_colours[l, 0]\n",
        "            g[temp == l] = label_colours[l, 1]\n",
        "            b[temp == l] = label_colours[l, 2]\n",
        "\n",
        "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "        rgb[:, :, 0] = r / 255.0\n",
        "        rgb[:, :, 1] = g / 255.0\n",
        "        rgb[:, :, 2] = b / 255.0\n",
        "        return rgb"
      ],
      "metadata": {
        "id": "PSzUwIOmpbrB"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing data loader\n",
        "local_path = \"data/\"\n",
        "#augmentations = Compose([RandomRotate(10), RandomHorizontallyFlip(0.5)])\n",
        "\n",
        "dst = camvidLoader(local_path, is_transform=True, augmentations=augmentations)\n",
        "bs = 4\n",
        "trainloader = data.DataLoader(dst, batch_size=bs, shuffle=True)\n",
        "for i, data_samples in enumerate(trainloader):\n",
        "    imgs, labels = data_samples\n",
        "    # print(imgs.size())\n",
        "    imgs = imgs.numpy()[:, ::-1, :, :]\n",
        "    imgs = np.transpose(imgs, [0, 2, 3, 1])\n",
        "    newlabels=torch.zeros((len(labels), 720 , 960))\n",
        "    # print(type(labels))\n",
        "    for i in range(len(labels)):\n",
        "      #print(labels[i,:,:,:].shape)\n",
        "      # print(labels[i, :, :, :].size(), imgs.shape)\n",
        "      newlabels[i] = rgb_to_lbl(labels[i,:,:,:])\n",
        "    labels = newlabels\n",
        "    f, axarr = plt.subplots(bs, 2)\n",
        "    for j in range(bs):\n",
        "        axarr[j][0].imshow(imgs[j])\n",
        "        axarr[j][1].imshow(dst.decode_segmap(newlabels.numpy()[j]))\n",
        "    plt.show()\n",
        "    a = input()\n",
        "    if a == \"ex\":\n",
        "        break\n",
        "    else:\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "7fjH7AL2p8Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import collections\n",
        "#from CatConv2d.catconv2d import CatConv2d\n",
        "\n",
        "class ConvLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n",
        "                                          stride=stride, padding=kernel//2, bias = False))\n",
        "        self.add_module('norm', nn.BatchNorm2d(out_channels))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "\n",
        "        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "        \n",
        "\n",
        "class BRLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.add_module('norm', nn.BatchNorm2d(in_channels))\n",
        "        self.add_module('relu', nn.ReLU(True))\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "\n",
        "class HarDBlock_v2(nn.Module):\n",
        "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
        "        if layer == 0:\n",
        "          return base_ch, 0, []\n",
        "        out_channels = growth_rate\n",
        "        link = []\n",
        "        for i in range(10):\n",
        "          dv = 2 ** i\n",
        "          if layer % dv == 0:\n",
        "            k = layer - dv\n",
        "            link.append(k)\n",
        "            if i > 0:\n",
        "                out_channels *= grmul\n",
        "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
        "        in_channels = 0\n",
        "        for i in link:\n",
        "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
        "          in_channels += ch\n",
        "        return out_channels, in_channels, link\n",
        "\n",
        "\n",
        "    def get_out_ch(self):\n",
        "        return self.out_channels\n",
        "\n",
        "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False, list_out=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.growth_rate = growth_rate\n",
        "        self.grmul = grmul\n",
        "        self.n_layers = n_layers\n",
        "        self.keepBase = keepBase\n",
        "        self.links = []\n",
        "        self.list_out = list_out\n",
        "        layers_ = []\n",
        "        self.out_channels = 0\n",
        "\n",
        "        for i in range(n_layers):\n",
        "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
        "          self.links.append(link)\n",
        "          use_relu = residual_out\n",
        "          #layers_.append(CatConv2d(inch, outch, (3,3), relu=True))\n",
        "          layers_.append(nn.Conv2d(inch, outch, (3,3), relu=True))\n",
        "\n",
        "          if (i % 2 == 0) or (i == n_layers - 1):\n",
        "            self.out_channels += outch\n",
        "        print(\"Blk out =\",self.out_channels)\n",
        "        self.layers = nn.ModuleList(layers_)\n",
        "\n",
        "    def transform(self, blk):\n",
        "        for i in range(len(self.layers)):\n",
        "            self.layers[i].weight[:,:,:,:] = blk.layers[i][0].weight[:,:,:,:]\n",
        "            self.layers[i].bias[:] = blk.layers[i][0].bias[:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers_ = [x]\n",
        "        #self.res = []\n",
        "        for layer in range(len(self.layers)):\n",
        "            link = self.links[layer]\n",
        "            tin = []\n",
        "            for i in link:\n",
        "                tin.append(layers_[i])\n",
        "\n",
        "            out = self.layers[layer](tin)\n",
        "            #self.res.append(out)\n",
        "            layers_.append(out)\n",
        "        t = len(layers_)\n",
        "        out_ = []\n",
        "        for i in range(t):\n",
        "          if (i == 0 and self.keepBase) or \\\n",
        "             (i == t-1) or (i%2 == 1):\n",
        "              out_.append(layers_[i])\n",
        "        if self.list_out:\n",
        "            return out_\n",
        "        else:\n",
        "            return torch.cat(out_, 1)\n",
        "\n",
        "\n",
        "\n",
        "class HarDBlock(nn.Module):\n",
        "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
        "        if layer == 0:\n",
        "          return base_ch, 0, []\n",
        "        out_channels = growth_rate\n",
        "        link = []\n",
        "        for i in range(10):\n",
        "          dv = 2 ** i\n",
        "          if layer % dv == 0:\n",
        "            k = layer - dv\n",
        "            link.append(k)\n",
        "            if i > 0:\n",
        "                out_channels *= grmul\n",
        "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
        "        in_channels = 0\n",
        "        for i in link:\n",
        "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
        "          in_channels += ch\n",
        "        return out_channels, in_channels, link\n",
        "\n",
        "    def get_out_ch(self):\n",
        "        return self.out_channels\n",
        " \n",
        "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.growth_rate = growth_rate\n",
        "        self.grmul = grmul\n",
        "        self.n_layers = n_layers\n",
        "        self.keepBase = keepBase\n",
        "        self.links = []\n",
        "        layers_ = []\n",
        "        self.out_channels = 0 # if upsample else in_channels\n",
        "        for i in range(n_layers):\n",
        "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
        "          self.links.append(link)\n",
        "          use_relu = residual_out\n",
        "          layers_.append(ConvLayer(inch, outch))\n",
        "          if (i % 2 == 0) or (i == n_layers - 1):\n",
        "            self.out_channels += outch\n",
        "        #print(\"Blk out =\",self.out_channels)\n",
        "        self.layers = nn.ModuleList(layers_)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        layers_ = [x]\n",
        "        for layer in range(len(self.layers)):\n",
        "            link = self.links[layer]\n",
        "            tin = []\n",
        "            for i in link:\n",
        "                tin.append(layers_[i])\n",
        "            if len(tin) > 1:\n",
        "                x = torch.cat(tin, 1)\n",
        "            else:\n",
        "                x = tin[0]\n",
        "            out = self.layers[layer](x)\n",
        "            layers_.append(out)\n",
        "        t = len(layers_)\n",
        "        out_ = []\n",
        "        for i in range(t):\n",
        "          if (i == 0 and self.keepBase) or \\\n",
        "             (i == t-1) or (i%2 == 1):\n",
        "              out_.append(layers_[i])\n",
        "        out = torch.cat(out_, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransitionUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        #print(\"upsample\",in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, skip, concat=True):\n",
        "        is_v2 = type(skip) is list\n",
        "        if is_v2:\n",
        "            skip_x = skip[0]\n",
        "        else:\n",
        "            skip_x = skip\n",
        "        out = F.interpolate(\n",
        "                x,\n",
        "                size=(skip_x.size(2), skip_x.size(3)),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=True,\n",
        "                            )\n",
        "        if concat:       \n",
        "          if is_v2:\n",
        "            out = [out] + skip\n",
        "          else:                     \n",
        "            out = torch.cat([out, skip], 1)\n",
        "          \n",
        "        return out\n",
        "\n",
        "class hardnet(nn.Module):\n",
        "    def __init__(self, n_classes=12):\n",
        "        super(hardnet, self).__init__()\n",
        "\n",
        "        first_ch  = [16,24,32,48]\n",
        "        ch_list = [  64, 96, 160, 224, 320]\n",
        "        grmul = 1.7\n",
        "        gr       = [  10,16,18,24,32]\n",
        "        n_layers = [   4, 4, 8, 8, 8]\n",
        "\n",
        "        blks = len(n_layers) \n",
        "        self.shortcut_layers = []\n",
        "\n",
        "        self.base = nn.ModuleList([])\n",
        "        self.base.append (\n",
        "             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n",
        "                       stride=2) )\n",
        "        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=3) )\n",
        "        self.base.append ( ConvLayer(first_ch[1], first_ch[2],  kernel=3, stride=2) )\n",
        "        self.base.append ( ConvLayer(first_ch[2], first_ch[3],  kernel=3) )\n",
        "\n",
        "        skip_connection_channel_counts = []\n",
        "        ch = first_ch[3]\n",
        "        for i in range(blks):\n",
        "            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n",
        "            ch = blk.get_out_ch()\n",
        "            skip_connection_channel_counts.append(ch)\n",
        "            self.base.append ( blk )\n",
        "            if i < blks-1:\n",
        "              self.shortcut_layers.append(len(self.base)-1)\n",
        "\n",
        "            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n",
        "            ch = ch_list[i]\n",
        "            \n",
        "            if i < blks-1:            \n",
        "              self.base.append ( nn.AvgPool2d(kernel_size=2, stride=2) )\n",
        "\n",
        "\n",
        "        cur_channels_count = ch\n",
        "        prev_block_channels = ch\n",
        "        n_blocks = blks-1\n",
        "        self.n_blocks =  n_blocks\n",
        "\n",
        "        # upsampling\n",
        "\n",
        "        self.transUpBlocks = nn.ModuleList([])\n",
        "        self.denseBlocksUp = nn.ModuleList([])\n",
        "        self.conv1x1_up    = nn.ModuleList([])\n",
        "        \n",
        "        for i in range(n_blocks-1,-1,-1):\n",
        "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
        "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
        "            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count//2, kernel=1))\n",
        "            cur_channels_count = cur_channels_count//2\n",
        "\n",
        "            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n",
        "            \n",
        "            self.denseBlocksUp.append(blk)\n",
        "            prev_block_channels = blk.get_out_ch()\n",
        "            cur_channels_count = prev_block_channels\n",
        "\n",
        "\n",
        "        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n",
        "               out_channels=n_classes, kernel_size=1, stride=1,\n",
        "               padding=0, bias=True)\n",
        "    \n",
        "    def v2_transform(self):        \n",
        "        for i in range( len(self.base)):\n",
        "            if isinstance(self.base[i], HarDBlock):\n",
        "                blk = self.base[i]\n",
        "                self.base[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=True)\n",
        "                self.base[i].transform(blk)\n",
        "            elif isinstance(self.base[i], nn.Sequential):\n",
        "                blk = self.base[i]\n",
        "                sz = blk[0].weight.shape\n",
        "                if sz[2] == 1:\n",
        "                    #self.base[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n",
        "                    self.base[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n",
        "                    self.base[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n",
        "                    self.base[i].bias[:] = blk[0].bias[:]\n",
        "\n",
        "        for i in range(self.n_blocks):\n",
        "            blk = self.denseBlocksUp[i]\n",
        "            self.denseBlocksUp[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=False)\n",
        "            self.denseBlocksUp[i].transform(blk)\n",
        "  \n",
        "        for i in range(len(self.conv1x1_up)):\n",
        "            blk = self.conv1x1_up[i]\n",
        "            sz = blk[0].weight.shape\n",
        "            if sz[2] == 1:\n",
        "                #self.conv1x1_up[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n",
        "                self.conv1x1_up[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n",
        "                self.conv1x1_up[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n",
        "                self.conv1x1_up[i].bias[:] = blk[0].bias[:]                 \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        skip_connections = []\n",
        "        size_in = x.size()\n",
        "        \n",
        "        \n",
        "        for i in range(len(self.base)):\n",
        "            x = self.base[i](x)\n",
        "            if i in self.shortcut_layers:\n",
        "                skip_connections.append(x)\n",
        "        out = x\n",
        "        \n",
        "        for i in range(self.n_blocks):\n",
        "            skip = skip_connections.pop()\n",
        "            out = self.transUpBlocks[i](out, skip, True)\n",
        "            out = self.conv1x1_up[i](out)\n",
        "            out = self.denseBlocksUp[i](out)\n",
        "        \n",
        "        out = self.finalConv(out)\n",
        "        \n",
        "        out = F.interpolate(\n",
        "                            out,\n",
        "                            size=(size_in[2], size_in[3]),\n",
        "                            mode=\"bilinear\",\n",
        "                            align_corners=True)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ip28mbfdrRiI"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class runningScore(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n",
        "        ).reshape(n_class, n_class)\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n",
        "\n",
        "    def get_scores(self):\n",
        "        \"\"\"Returns accuracy score evaluation result.\n",
        "            - overall accuracy\n",
        "            - mean accuracy\n",
        "            - mean IU\n",
        "            - fwavacc\n",
        "        \"\"\"\n",
        "        hist = self.confusion_matrix\n",
        "        acc = np.diag(hist).sum() / hist.sum()\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "        acc_cls = np.nanmean(acc_cls)\n",
        "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "        mean_iu = np.nanmean(iu)\n",
        "        freq = hist.sum(axis=1) / hist.sum()\n",
        "        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        cls_iu = dict(zip(range(self.n_classes), iu))\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"Overall Acc: \\t\": acc,\n",
        "                \"Mean Acc : \\t\": acc_cls,\n",
        "                \"FreqW Acc : \\t\": fwavacc,\n",
        "                \"Mean IoU : \\t\": mean_iu,\n",
        "            },\n",
        "            cls_iu,\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
        "\n",
        "\n",
        "class averageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "metadata": {
        "id": "CTH2_3VHSPUP"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
        "    n, c, h, w = input.size()\n",
        "    nt, ht, wt = target.size()\n",
        "    #print(input.size(), target.size())\n",
        "    #nt, ht, wt = target.size()\n",
        "\n",
        "    if h != ht and w != wt:  # upsample labels\n",
        "        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
        "    target = target.view(-1)\n",
        "    #print(input.size(), target.size())\n",
        "    #print(input[0, 0].dtype, target[0].dtype)\n",
        "    loss = F.cross_entropy(\n",
        "              input, target, weight=weight, size_average=size_average, ignore_index=250, reduction='mean')\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "3qQA2XVLswhK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "# Setup seeds\n",
        "torch.manual_seed(1337)\n",
        "torch.cuda.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "random.seed(1337)\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Setup Augmentations\n",
        "#augmentations = cfg[\"training\"].get(\"augmentations\", None)\n",
        "#data_aug = get_composed_augmentations(augmentations)\n",
        "data_aug = Compose([RandomRotate(10), RandomHorizontallyFlip(0.5)])\n",
        "\n",
        "# Setup Dataloader\n",
        "\n",
        "#data_loader = get_loader(cfg[\"data\"][\"dataset\"])\n",
        "data_loader = camvidLoader\n",
        "#data_path = cfg[\"data\"][\"path\"]\n",
        "data_path = 'data/'\n",
        "\n",
        "t_loader = data_loader(\n",
        "    data_path,\n",
        "    is_transform=True,\n",
        "    #split=cfg[\"data\"][\"train_split\"],\n",
        "    split='train',\n",
        "    #img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n",
        "    img_size=(720, 960),\n",
        "    augmentations=data_aug,\n",
        ")\n",
        "\n",
        "v_loader = data_loader(\n",
        "    data_path,\n",
        "    is_transform=True,\n",
        "    #split=cfg[\"data\"][\"val_split\"],\n",
        "    split='val',\n",
        "    img_size=(720,960),\n",
        ")\n",
        "\n",
        "n_classes = t_loader.n_classes\n",
        "trainloader = data.DataLoader(\n",
        "    t_loader,\n",
        "    #batch_size=cfg[\"training\"][\"batch_size\"],\n",
        "    batch_size=4,\n",
        "    #num_workers=cfg[\"training\"][\"n_workers\"],\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "valloader = data.DataLoader(\n",
        "    v_loader, \n",
        "    #batch_size=cfg[\"training\"][\"batch_size\"],\n",
        "    batch_size=4, \n",
        "    #num_workers=cfg[\"training\"][\"n_workers\"]\n",
        ")\n",
        "\n",
        "# Setup Metrics\n",
        "running_metrics_val = runningScore(n_classes)\n",
        "\n",
        "# Setup Model\n",
        "\n",
        "#model = get_model(cfg[\"model\"], n_classes).to(device)\n",
        "model = hardnet()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print( 'Parameters:',total_params )\n",
        "\n",
        "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
        "model.apply(weights_init)\n",
        "#pretrained_path='weights/hardnet_petite_base.pth'\n",
        "#weights = torch.load(pretrained_path)\n",
        "#model.module.base.load_state_dict(weights)\n",
        "\n",
        "# Setup optimizer, lr_scheduler and loss function\n",
        "#optimizer_cls = get_optimizer(cfg)\n",
        "optimizer_cls = torch.optim.SGD\n",
        "#optimizer_params = {k: v for k, v in cfg[\"training\"][\"optimizer\"].items() if k != \"name\"}\n",
        "\n",
        "optimizer = optimizer_cls(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "print(\"Using optimizer {}\".format(optimizer))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "#loss_fn = get_loss_function(cfg)\n",
        "loss_fn = cross_entropy2d\n",
        "print(\"Using loss {}\".format(loss_fn))\n",
        "\n",
        "start_iter = 0\n",
        "# if cfg[\"training\"][\"resume\"] is not None:\n",
        "#     if os.path.isfile(cfg[\"training\"][\"resume\"]):\n",
        "#         logger.info(\n",
        "#             \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"resume\"])\n",
        "#         )\n",
        "#         checkpoint = torch.load(cfg[\"training\"][\"resume\"])\n",
        "#         model.load_state_dict(checkpoint[\"model_state\"])\n",
        "#         optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "#         scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "#         start_iter = checkpoint[\"epoch\"]\n",
        "#         logger.info(\n",
        "#             \"Loaded checkpoint '{}' (iter {})\".format(\n",
        "#                 cfg[\"training\"][\"resume\"], checkpoint[\"epoch\"]\n",
        "#             )\n",
        "#         )\n",
        "#     else:\n",
        "#         logger.info(\"No checkpoint found at '{}'\".format(cfg[\"training\"][\"resume\"]))\n",
        "\n",
        "# if cfg[\"training\"][\"finetune\"] is not None:\n",
        "#     if os.path.isfile(cfg[\"training\"][\"finetune\"]):\n",
        "#         logger.info(\n",
        "#             \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"finetune\"])\n",
        "#         )\n",
        "#         checkpoint = torch.load(cfg[\"training\"][\"finetune\"])\n",
        "#         model.load_state_dict(checkpoint[\"model_state\"])\n",
        "\n",
        "val_loss_meter = averageMeter()\n",
        "time_meter = averageMeter()\n",
        "\n",
        "best_iou = -100.0\n",
        "flag = True\n",
        "loss_all = 0\n",
        "loss_n = 0\n",
        "epoch = 0\n",
        "num_epochs = 10\n",
        "per_epoch = 200\n",
        "train_iters = 2000\n",
        "i = start_iter\n",
        "#while i <= cfg[\"training\"][\"train_iters\"] and flag:\n",
        "while i <= train_iters and flag:\n",
        "    for (images, labels) in trainloader:\n",
        "        i += 1\n",
        "        start_ts = time.time()\n",
        "        model.train()\n",
        "        images = images.to(device)\n",
        "        newlabels = torch.zeros((len(labels), 720, 960))\n",
        "        for j in range(len(labels)):\n",
        "          newlabels[j] = rgb_to_lbl(labels[j, :, :, :])\n",
        "        labels = newlabels\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        #print(images.size(), outputs.size(), labels.size())\n",
        "\n",
        "        #print(labels.size())\n",
        "        # print(labels.count_nonzero(), labels.size())\n",
        "        loss = loss_fn(input=outputs, target=labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        c_lr = scheduler.get_last_lr()\n",
        "\n",
        "        time_meter.update(time.time() - start_ts)\n",
        "        loss_all += loss.item()\n",
        "        loss_n += 1\n",
        "        \n",
        "        #if (i + 1) % cfg[\"training\"][\"print_interval\"] == 0:\n",
        "        if (i + 1) % per_epoch == 0:\n",
        "          epoch = epoch + 1\n",
        "          scheduler.step()\n",
        "        if (i + 1) % 10 == 0:\n",
        "            fmt_str = \"Epoch {:d} Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}  lr={:.6f}\"\n",
        "            print_str = fmt_str.format(\n",
        "                epoch,\n",
        "                (i + 1) % (train_iters // num_epochs),\n",
        "                #cfg[\"training\"][\"train_iters\"],\n",
        "                train_iters // num_epochs,\n",
        "                loss_all / loss_n,\n",
        "                #time_meter.avg / cfg[\"training\"][\"batch_size\"],\n",
        "                time_meter.avg / 4,\n",
        "                c_lr[0],\n",
        "            )\n",
        "            \n",
        "\n",
        "            print(print_str)\n",
        "            #logger.info(print_str)\n",
        "            #writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n",
        "            time_meter.reset()\n",
        "\n",
        "        # if (i + 1) % cfg[\"training\"][\"val_interval\"] == 0 or (i + 1) == cfg[\"training\"][\n",
        "        #     \"train_iters\"\n",
        "        # ]:\n",
        "        #print(i + 1)\n",
        "        if (i + 1) % per_epoch == 0 or (i + 1) == train_iters:\n",
        "          torch.cuda.empty_cache()\n",
        "          model.eval()\n",
        "          loss_all = 0\n",
        "          loss_n = 0\n",
        "          with torch.no_grad():\n",
        "              for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
        "                  images_val = images_val.to(device)\n",
        "                  newlabels = torch.zeros((len(labels_val), 720, 960))\n",
        "                  for j in range(len(labels_val)):\n",
        "                    newlabels[j] = rgb_to_lbl(labels_val[j, :, :, :])\n",
        "                  labels_val = newlabels\n",
        "                  labels_val = labels_val.to(device)\n",
        "\n",
        "                  outputs = model(images_val)\n",
        "                  val_loss = loss_fn(input=outputs, target=labels_val.long())\n",
        "\n",
        "                  pred = outputs.data.max(1)[1].cpu().numpy()\n",
        "                  gt = labels_val.data.cpu().numpy()\n",
        "\n",
        "                  running_metrics_val.update(gt, pred)\n",
        "                  val_loss_meter.update(val_loss.item())\n",
        "\n",
        "          #writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
        "          #logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
        "          print(format(\"Epoch %d Iter %d Val Loss: %.4f\" % (epoch, i + 1, val_loss_meter.avg)))\n",
        "\n",
        "          score, class_iou = running_metrics_val.get_scores()\n",
        "          for k, v in score.items():\n",
        "              print(k, v)\n",
        "              #logger.info(\"{}: {}\".format(k, v))\n",
        "              #writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n",
        "\n",
        "          #for k, v in class_iou.items():\n",
        "              #logger.info(\"{}: {}\".format(k, v))\n",
        "              #writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n",
        "\n",
        "          val_loss_meter.reset()\n",
        "          running_metrics_val.reset()\n",
        "          \n",
        "          state = {\n",
        "                \"epoch\": i + 1,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"scheduler_state\": scheduler.state_dict(),\n",
        "          }\n",
        "          # save_path = os.path.join(\n",
        "          #     writer.file_writer.get_logdir(),\n",
        "          #     \"{}_{}_checkpoint.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n",
        "          # )\n",
        "          # torch.save(state, save_path)\n",
        "\n",
        "          if score[\"Mean IoU : \\t\"] >= best_iou:\n",
        "              best_iou = score[\"Mean IoU : \\t\"]\n",
        "              state = {\n",
        "                  \"epoch\": i + 1,\n",
        "                  \"model_state\": model.state_dict(),\n",
        "                  \"best_iou\": best_iou,\n",
        "              }\n",
        "              # save_path = os.path.join(\n",
        "              #     writer.file_writer.get_logdir(),\n",
        "              #     \"{}_{}_best_model.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n",
        "              # )\n",
        "              # torch.save(state, save_path)\n",
        "          torch.cuda.empty_cache()\n",
        "      \n",
        "          # if (i + 1) == cfg[\"training\"][\"train_iters\"]:\n",
        "    if (i + 1) == 90000:\n",
        "        flag = False\n",
        "        break"
      ],
      "metadata": {
        "id": "Iew4MIgdyn9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64b96530-9346-41e4-c70a-ddeb6ff989c9"
      },
      "execution_count": 108,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: 4118914\n",
            "Using optimizer SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            "Using loss <function cross_entropy2d at 0x7f1b037f2710>\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:1132: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed in 0.14. Please use 'interpolation' instead.\n",
            "  \"The parameter 'resample' is deprecated since 0.12 and will be removed in 0.14. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Iter [10/100]  Loss: 1.8207  Time/Image: 2.4784  lr=0.100000\n",
            "Epoch 0 Iter [20/100]  Loss: 1.5023  Time/Image: 2.4579  lr=0.100000\n",
            "Epoch 0 Iter [30/100]  Loss: 1.3984  Time/Image: 2.4611  lr=0.100000\n",
            "Epoch 0 Iter [40/100]  Loss: 1.3121  Time/Image: 2.5257  lr=0.100000\n",
            "Epoch 0 Iter [50/100]  Loss: 1.2591  Time/Image: 2.4545  lr=0.100000\n",
            "Epoch 0 Iter [60/100]  Loss: 1.2159  Time/Image: 2.4509  lr=0.100000\n",
            "Epoch 0 Iter [70/100]  Loss: 1.1788  Time/Image: 2.4423  lr=0.100000\n",
            "Epoch 0 Iter [80/100]  Loss: 1.1525  Time/Image: 2.4562  lr=0.100000\n",
            "Epoch 0 Iter [90/100]  Loss: 1.1289  Time/Image: 2.4405  lr=0.100000\n",
            "Epoch 1 Iter [0/100]  Loss: 1.1132  Time/Image: 2.4368  lr=0.100000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18it [01:23,  4.65s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 100 Val Loss: 0.9728\n",
            "Overall Acc: \t 0.6345485703573291\n",
            "Mean Acc : \t 0.3773381916211499\n",
            "FreqW Acc : \t 0.46355850414288957\n",
            "Mean IoU : \t 0.2743992698622338\n",
            "Epoch 1 Iter [10/100]  Loss: 0.9304  Time/Image: 2.4333  lr=0.050000\n",
            "Epoch 1 Iter [20/100]  Loss: 0.8965  Time/Image: 2.4250  lr=0.050000\n",
            "Epoch 1 Iter [30/100]  Loss: 0.8860  Time/Image: 2.4259  lr=0.050000\n",
            "Epoch 1 Iter [40/100]  Loss: 0.8990  Time/Image: 2.4187  lr=0.050000\n",
            "Epoch 1 Iter [50/100]  Loss: 0.8942  Time/Image: 2.4070  lr=0.050000\n",
            "Epoch 1 Iter [60/100]  Loss: 0.8876  Time/Image: 2.4163  lr=0.050000\n",
            "Epoch 1 Iter [70/100]  Loss: 0.8844  Time/Image: 2.5175  lr=0.050000\n",
            "Epoch 1 Iter [80/100]  Loss: 0.8759  Time/Image: 2.4235  lr=0.050000\n",
            "Epoch 1 Iter [90/100]  Loss: 0.8705  Time/Image: 2.4199  lr=0.050000\n",
            "Epoch 2 Iter [0/100]  Loss: 0.8653  Time/Image: 2.4288  lr=0.050000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18it [01:21,  4.54s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Iter 200 Val Loss: 1.0189\n",
            "Overall Acc: \t 0.6222748557316119\n",
            "Mean Acc : \t 0.4355760556905455\n",
            "FreqW Acc : \t 0.4728169512166409\n",
            "Mean IoU : \t 0.3164234497703723\n",
            "Epoch 2 Iter [10/100]  Loss: 0.7667  Time/Image: 2.4144  lr=0.025000\n",
            "Epoch 2 Iter [20/100]  Loss: 0.7699  Time/Image: 2.5192  lr=0.025000\n",
            "Epoch 2 Iter [30/100]  Loss: 0.8091  Time/Image: 2.4486  lr=0.025000\n",
            "Epoch 2 Iter [40/100]  Loss: 0.8213  Time/Image: 2.4475  lr=0.025000\n",
            "Epoch 2 Iter [50/100]  Loss: 0.8119  Time/Image: 2.4462  lr=0.025000\n",
            "Epoch 2 Iter [60/100]  Loss: 0.8114  Time/Image: 2.4427  lr=0.025000\n",
            "Epoch 2 Iter [70/100]  Loss: 0.8022  Time/Image: 2.4421  lr=0.025000\n",
            "Epoch 2 Iter [80/100]  Loss: 0.7962  Time/Image: 2.4411  lr=0.025000\n",
            "Epoch 2 Iter [90/100]  Loss: 0.7868  Time/Image: 2.4443  lr=0.025000\n",
            "Epoch 3 Iter [0/100]  Loss: 0.7818  Time/Image: 2.4433  lr=0.025000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18it [01:23,  4.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Iter 300 Val Loss: 0.7833\n",
            "Overall Acc: \t 0.7243008281168493\n",
            "Mean Acc : \t 0.5151683083191397\n",
            "FreqW Acc : \t 0.5795565976080395\n",
            "Mean IoU : \t 0.40843286436255977\n",
            "Epoch 3 Iter [10/100]  Loss: 0.7906  Time/Image: 2.4399  lr=0.012500\n",
            "Epoch 3 Iter [20/100]  Loss: 0.7464  Time/Image: 2.4529  lr=0.012500\n",
            "Epoch 3 Iter [30/100]  Loss: 0.7249  Time/Image: 2.5095  lr=0.012500\n",
            "Epoch 3 Iter [40/100]  Loss: 0.7287  Time/Image: 2.4741  lr=0.012500\n",
            "Epoch 3 Iter [50/100]  Loss: 0.7210  Time/Image: 2.4657  lr=0.012500\n",
            "Epoch 3 Iter [60/100]  Loss: 0.7201  Time/Image: 2.4553  lr=0.012500\n",
            "Epoch 3 Iter [70/100]  Loss: 0.7147  Time/Image: 2.4431  lr=0.012500\n",
            "Epoch 3 Iter [80/100]  Loss: 0.7157  Time/Image: 2.4439  lr=0.012500\n",
            "Epoch 3 Iter [90/100]  Loss: 0.7156  Time/Image: 2.4295  lr=0.012500\n",
            "Epoch 4 Iter [0/100]  Loss: 0.7102  Time/Image: 2.4335  lr=0.012500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18it [01:22,  4.60s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Iter 400 Val Loss: 0.7351\n",
            "Overall Acc: \t 0.7516860654668753\n",
            "Mean Acc : \t 0.5478513395290026\n",
            "FreqW Acc : \t 0.6127915312795584\n",
            "Mean IoU : \t 0.4401749083521877\n",
            "Epoch 4 Iter [10/100]  Loss: 0.6158  Time/Image: 2.4298  lr=0.006250\n",
            "Epoch 4 Iter [20/100]  Loss: 0.6367  Time/Image: 2.4282  lr=0.006250\n",
            "Epoch 4 Iter [30/100]  Loss: 0.6485  Time/Image: 2.4265  lr=0.006250\n",
            "Epoch 4 Iter [40/100]  Loss: 0.6494  Time/Image: 2.4137  lr=0.006250\n",
            "Epoch 4 Iter [50/100]  Loss: 0.6561  Time/Image: 2.4097  lr=0.006250\n",
            "Epoch 4 Iter [60/100]  Loss: 0.6493  Time/Image: 2.4082  lr=0.006250\n",
            "Epoch 4 Iter [70/100]  Loss: 0.6465  Time/Image: 2.4149  lr=0.006250\n",
            "Epoch 4 Iter [80/100]  Loss: 0.6451  Time/Image: 2.4182  lr=0.006250\n",
            "Epoch 4 Iter [90/100]  Loss: 0.6442  Time/Image: 2.3943  lr=0.006250\n",
            "Epoch 5 Iter [0/100]  Loss: 0.6452  Time/Image: 2.3991  lr=0.006250\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18it [01:20,  4.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Iter 500 Val Loss: 0.6769\n",
            "Overall Acc: \t 0.774119005118675\n",
            "Mean Acc : \t 0.5787546626881355\n",
            "FreqW Acc : \t 0.644308747176457\n",
            "Mean IoU : \t 0.47270997189493963\n",
            "Epoch 5 Iter [10/100]  Loss: 0.6372  Time/Image: 2.5234  lr=0.003125\n",
            "Epoch 5 Iter [20/100]  Loss: 0.6346  Time/Image: 2.4390  lr=0.003125\n",
            "Epoch 5 Iter [30/100]  Loss: 0.6407  Time/Image: 2.4306  lr=0.003125\n",
            "Epoch 5 Iter [40/100]  Loss: 0.6348  Time/Image: 2.4223  lr=0.003125\n",
            "Epoch 5 Iter [50/100]  Loss: 0.6338  Time/Image: 2.4163  lr=0.003125\n",
            "Epoch 5 Iter [60/100]  Loss: 0.6250  Time/Image: 2.4100  lr=0.003125\n",
            "Epoch 5 Iter [70/100]  Loss: 0.6219  Time/Image: 2.4148  lr=0.003125\n",
            "Epoch 5 Iter [80/100]  Loss: 0.6250  Time/Image: 2.4025  lr=0.003125\n",
            "Epoch 5 Iter [90/100]  Loss: 0.6259  Time/Image: 2.4069  lr=0.003125\n",
            "Epoch 6 Iter [0/100]  Loss: 0.6234  Time/Image: 2.4061  lr=0.003125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [01:21,  4.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Iter 600 Val Loss: 0.6730\n",
            "Overall Acc: \t 0.7791694175469484\n",
            "Mean Acc : \t 0.5815055363519472\n",
            "FreqW Acc : \t 0.6516446742413634\n",
            "Mean IoU : \t 0.4763048366075632\n",
            "Epoch 6 Iter [10/100]  Loss: 0.6290  Time/Image: 2.4021  lr=0.001563\n",
            "Epoch 6 Iter [20/100]  Loss: 0.6043  Time/Image: 2.4044  lr=0.001563\n",
            "Epoch 6 Iter [30/100]  Loss: 0.6019  Time/Image: 2.4024  lr=0.001563\n",
            "Epoch 6 Iter [40/100]  Loss: 0.6072  Time/Image: 2.4064  lr=0.001563\n",
            "Epoch 6 Iter [50/100]  Loss: 0.6173  Time/Image: 2.4054  lr=0.001563\n",
            "Epoch 6 Iter [60/100]  Loss: 0.6102  Time/Image: 2.3956  lr=0.001563\n",
            "Epoch 6 Iter [70/100]  Loss: 0.6087  Time/Image: 2.3913  lr=0.001563\n",
            "Epoch 6 Iter [80/100]  Loss: 0.6059  Time/Image: 2.3954  lr=0.001563\n",
            "Epoch 6 Iter [90/100]  Loss: 0.6037  Time/Image: 2.3939  lr=0.001563\n",
            "Epoch 7 Iter [0/100]  Loss: 0.6051  Time/Image: 2.5090  lr=0.001563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [01:22,  4.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Iter 700 Val Loss: 0.6481\n",
            "Overall Acc: \t 0.789150609676578\n",
            "Mean Acc : \t 0.592767537681298\n",
            "FreqW Acc : \t 0.6639414025558772\n",
            "Mean IoU : \t 0.485854099753789\n",
            "Epoch 7 Iter [10/100]  Loss: 0.5716  Time/Image: 2.4312  lr=0.000781\n",
            "Epoch 7 Iter [20/100]  Loss: 0.5824  Time/Image: 2.4339  lr=0.000781\n",
            "Epoch 7 Iter [30/100]  Loss: 0.5944  Time/Image: 2.4319  lr=0.000781\n",
            "Epoch 7 Iter [40/100]  Loss: 0.5855  Time/Image: 2.4248  lr=0.000781\n",
            "Epoch 7 Iter [50/100]  Loss: 0.5791  Time/Image: 2.4181  lr=0.000781\n",
            "Epoch 7 Iter [60/100]  Loss: 0.5865  Time/Image: 2.4140  lr=0.000781\n",
            "Epoch 7 Iter [70/100]  Loss: 0.5918  Time/Image: 2.4179  lr=0.000781\n",
            "Epoch 7 Iter [80/100]  Loss: 0.5914  Time/Image: 2.4213  lr=0.000781\n",
            "Epoch 7 Iter [90/100]  Loss: 0.5965  Time/Image: 2.4114  lr=0.000781\n",
            "Epoch 8 Iter [0/100]  Loss: 0.5975  Time/Image: 2.4026  lr=0.000781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [01:21,  4.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Iter 800 Val Loss: 0.6465\n",
            "Overall Acc: \t 0.7912087164188837\n",
            "Mean Acc : \t 0.5905320683187504\n",
            "FreqW Acc : \t 0.667640882127459\n",
            "Mean IoU : \t 0.4863786208212329\n",
            "Epoch 8 Iter [10/100]  Loss: 0.5955  Time/Image: 2.3934  lr=0.000391\n",
            "Epoch 8 Iter [20/100]  Loss: 0.6041  Time/Image: 2.3943  lr=0.000391\n",
            "Epoch 8 Iter [30/100]  Loss: 0.6099  Time/Image: 2.3935  lr=0.000391\n",
            "Epoch 8 Iter [40/100]  Loss: 0.6124  Time/Image: 2.3906  lr=0.000391\n",
            "Epoch 8 Iter [50/100]  Loss: 0.6047  Time/Image: 2.3906  lr=0.000391\n",
            "Epoch 8 Iter [70/100]  Loss: 0.5990  Time/Image: 2.3901  lr=0.000391\n",
            "Epoch 8 Iter [80/100]  Loss: 0.5985  Time/Image: 2.3888  lr=0.000391\n",
            "Epoch 8 Iter [90/100]  Loss: 0.5990  Time/Image: 2.3889  lr=0.000391\n",
            "Epoch 9 Iter [0/100]  Loss: 0.5986  Time/Image: 2.4223  lr=0.000391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [01:21,  4.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Iter 900 Val Loss: 0.6349\n",
            "Overall Acc: \t 0.795669890290819\n",
            "Mean Acc : \t 0.5923947713613976\n",
            "FreqW Acc : \t 0.671391714213898\n",
            "Mean IoU : \t 0.4910015155930586\n",
            "Epoch 9 Iter [10/100]  Loss: 0.5825  Time/Image: 2.4190  lr=0.000195\n",
            "Epoch 9 Iter [20/100]  Loss: 0.5990  Time/Image: 2.4209  lr=0.000195\n",
            "Epoch 9 Iter [30/100]  Loss: 0.6067  Time/Image: 2.4258  lr=0.000195\n",
            "Epoch 9 Iter [40/100]  Loss: 0.6028  Time/Image: 2.4288  lr=0.000195\n",
            "Epoch 9 Iter [50/100]  Loss: 0.6041  Time/Image: 2.4159  lr=0.000195\n",
            "Epoch 9 Iter [60/100]  Loss: 0.6010  Time/Image: 2.4158  lr=0.000195\n",
            "Epoch 9 Iter [70/100]  Loss: 0.6005  Time/Image: 2.4164  lr=0.000195\n",
            "Epoch 9 Iter [80/100]  Loss: 0.5987  Time/Image: 2.4203  lr=0.000195\n",
            "Epoch 9 Iter [90/100]  Loss: 0.5976  Time/Image: 2.4203  lr=0.000195\n",
            "Epoch 10 Iter [0/100]  Loss: 0.5984  Time/Image: 2.4060  lr=0.000195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [01:21,  4.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Iter 1000 Val Loss: 0.6365\n",
            "Overall Acc: \t 0.7948272854720918\n",
            "Mean Acc : \t 0.591980630906924\n",
            "FreqW Acc : \t 0.6710754558820888\n",
            "Mean IoU : \t 0.48997835455654937\n",
            "Epoch 10 Iter [10/100]  Loss: 0.5821  Time/Image: 2.4206  lr=0.000098\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-a90bc6c39dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m#print(images.size(), outputs.size(), labels.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-edd4613a5c2c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 623\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "FCHarDNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11QveP7cHwnrVuG57zkymf8zjeBoJk16x",
      "authorship_tag": "ABX9TyNhdjTBnHAEwkubgJEL5vWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}