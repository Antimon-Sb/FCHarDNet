{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport numbers\nimport random\nimport torchvision.transforms.functional as tf\n\nfrom PIL import Image, ImageOps\n\n\nclass Compose(object):\n    def __init__(self, augmentations):\n        self.augmentations = augmentations\n        self.PIL2Numpy = False\n\n    def __call__(self, img, mask):\n        if isinstance(img, np.ndarray):\n            img = Image.fromarray(img, mode=\"RGB\")\n            mask = Image.fromarray(mask, mode=\"RGB\")\n            self.PIL2Numpy = True\n\n        assert img.size == mask.size\n        for a in self.augmentations:\n            img, mask = a(img, mask)\n\n        if self.PIL2Numpy:\n            img, mask = np.array(img), np.array(mask, dtype=np.uint8)\n\n        return img, mask\n\nclass RandomHorizontallyFlip(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            return (img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT))\n        return img, mask\n        \nclass RandomScaleCrop(object):\n    def __init__(self, size):\n        self.size = size\n        self.crop = RandomCrop(self.size)\n            \n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        r = random.uniform(0.5, 2.0)\n        w, h = img.size\n        new_size = (int(w*r),int(h*r))\n        return self.crop(img.resize(new_size, Image.BILINEAR), mask.resize(new_size, Image.NEAREST))\n    \nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        ch, cw = self.size\n        if w == cw and h == ch:\n            return img, mask\n        if w < cw or h < ch:\n            pw = cw - w if cw > w else 0\n            ph = ch - h if ch > h else 0\n            padding = (pw,ph,pw,ph)\n            img  = ImageOps.expand(img,  padding, fill=0)\n            mask = ImageOps.expand(mask, padding, fill=250)\n            w, h = img.size\n            assert img.size == mask.size\n            \n        x1 = random.randint(0, w - cw)\n        y1 = random.randint(0, h - ch)\n        return (img.crop((x1, y1, x1 + cw, y1 + ch)), mask.crop((x1, y1, x1 + cw, y1 + ch)))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-24T10:52:48.268504Z","iopub.execute_input":"2022-05-24T10:52:48.268952Z","iopub.status.idle":"2022-05-24T10:52:48.289221Z","shell.execute_reply.started":"2022-05-24T10:52:48.268917Z","shell.execute_reply":"2022-05-24T10:52:48.288510Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from traitlets.traitlets import Long\n#data loader\nimport collections\nimport torch\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nimport skimage.transform\nfrom torch.utils import data\n\n\ndef rgb_to_lbl(rgb, size):\n    Sky = [128, 128, 128]\n    Building = [128, 0, 0]\n    Pole = [192, 192, 128]\n    Road = [128, 64, 128]\n    Pavement = [0, 0, 192]\n    Tree = [128, 128, 0]\n    SignSymbol = [192, 128, 128]\n    Fence = [64, 64, 128]\n    Car = [64, 0, 128]\n    Pedestrian = [64, 64, 0]\n    Bicyclist = [0, 128, 192]\n#     Unlabelled = [0, 0, 0]\n\n    label_colours = np.array(\n        [\n            Sky,\n            Building,\n            Pole,\n            Road,\n            Pavement,\n            Tree,\n            SignSymbol,\n            Fence,\n            Car,\n            Pedestrian,\n            Bicyclist,\n#             Unlabelled,\n        ]\n    )\n    label = 11 * torch.ones(size[0] * size[1], dtype=torch.uint8)\n    w, h, t = rgb.size()\n    rgb = rgb.reshape(-1, t)\n    for l in range(0, len(label_colours)):\n      r = rgb[:, 0] == label_colours[l, 0]\n      g = rgb[:, 1] == label_colours[l, 1]\n      b = rgb[:, 2] == label_colours[l, 2]\n      tf_rgb = r * g * b\n\n      label[tf_rgb] = l\n    label = label.reshape(size[0], size[1])\n    #print(rgb2[360][260:270], label[360][260:270])\n    return label\n\nclass camvidLoader(data.Dataset):\n    def __init__(\n        self,\n        root,\n        split=\"train\",\n        is_transform=False,\n        img_size=None,\n        augmentations=None,\n        img_norm=True,\n        test_mode=False\n    ):\n        self.root = root\n        self.split = split\n        self.img_size = img_size\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n        self.img_norm = img_norm\n        self.test_mode = test_mode\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 11\n        self.files = collections.defaultdict(list)\n\n        if not self.test_mode:\n            for split in [\"train\", \"test\", \"val\"]:\n                file_list = os.listdir(root + \"/\" + split)\n                self.files[split] = file_list\n            self.files['train'] += self.files['val'][:]\n            del self.files['val']\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        img_name = self.files[self.split][index]\n        if os.path.exists(self.root + \"/\" + self.split + \"/\" + img_name):\n            img_path = self.root + \"/\" + self.split + \"/\" + img_name\n            lbl_path = self.root + \"/\" + self.split + \"_labels/\" + img_name.split('.')[0] + '_L.' + img_name.split('.')[1]\n        else:\n            img_path = self.root + \"/\" + 'val' + \"/\" + img_name\n            lbl_path = self.root + \"/\" + 'val' + \"_labels/\" + img_name.split('.')[0] + '_L.' + img_name.split('.')[1]\n        img = plt.imread(img_path)\n        img = np.array(img*255, dtype=np.uint8)\n\n        lbl = plt.imread(lbl_path)\n        lbl = np.array(lbl*255, dtype=np.uint8)\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n        return img, lbl\n\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]  # RGB -> BGR\n        img = img.astype(np.float64)\n        img -= self.mean\n        if self.img_norm:\n            # Resize scales images from 0 to 255, thus we need\n            # to divide by 255.0\n            img = img.astype(float) / 255.0\n        # NHWC -> NCHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        Sky = [128, 128, 128]\n        Building = [128, 0, 0]\n        Pole = [192, 192, 128]\n        Road = [128, 64, 128]\n        Pavement = [0, 0, 192]\n        Tree = [128, 128, 0]\n        SignSymbol = [192, 128, 128]\n        Fence = [64, 64, 128]\n        Car = [64, 0, 128]\n        Pedestrian = [64, 64, 0]\n        Bicyclist = [0, 128, 192]\n        # Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Sky,\n                Building,\n                Pole,\n                Road,\n                Pavement,\n                Tree,\n                SignSymbol,\n                Fence,\n                Car,\n                Pedestrian,\n                Bicyclist,\n                # Unlabelled,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n        rgb[:, :, 0] = r / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = b / 255.0\n        return rgb","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:52:48.291166Z","iopub.execute_input":"2022-05-24T10:52:48.291581Z","iopub.status.idle":"2022-05-24T10:52:48.332553Z","shell.execute_reply.started":"2022-05-24T10:52:48.291546Z","shell.execute_reply":"2022-05-24T10:52:48.331649Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# #testing data loader\n# local_path = \"../input/camvid/CamVid\"\n# augmentations = Compose([RandomHorizontallyFlip(0.5), \n#                          RandomScaleCrop((448, 448))\n#                          ])\n\n# dst = camvidLoader(local_path,img_size=[720,960], is_transform=True, augmentations=augmentations)\n# for k, v in dst.files.items():\n#     print(k, len(v))\n# bs = 4\n# trainloader = data.DataLoader(dst, batch_size=bs, shuffle=True)\n# for i, data_samples in enumerate(trainloader):\n#     imgs, labels = data_samples\n#     # print(imgs.size())\n#     imgs = imgs.numpy()[:, ::-1, :, :]\n#     imgs = np.transpose(imgs, [0, 2, 3, 1])\n#     newlabels=torch.zeros((labels.shape[0], labels.shape[1], labels.shape[2]))\n#     # print(type(labels))\n#     for i in range(len(labels)):\n#       #print(labels[i,:,:,:].shape)\n#       # print(labels[i, :, :, :].size(), imgs.shape)\n#       newlabels[i] = rgb_to_lbl(labels[i,:,:,:], [labels.shape[1], labels.shape[2]])\n#     labels = newlabels\n#     f, axarr = plt.subplots(bs, 2)\n#     for j in range(bs):\n#         #print(imgs[j].shape, labels.shape)\n#         axarr[j][0].imshow(imgs[j])\n#         axarr[j][1].imshow(dst.decode_segmap(labels.numpy()[j]))\n#     plt.show()\n#     a = input()\n#     if a == \"ex\":\n#         break\n#     else:\n#         plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:52:48.334023Z","iopub.execute_input":"2022-05-24T10:52:48.334394Z","iopub.status.idle":"2022-05-24T10:52:48.347391Z","shell.execute_reply.started":"2022-05-24T10:52:48.334357Z","shell.execute_reply":"2022-05-24T10:52:48.346514Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\n\nclass ConvLayer(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1):\n        super().__init__()\n        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n                                          stride=stride, padding=kernel//2, bias = False))\n        self.add_module('norm', nn.BatchNorm2d(out_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n\n        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n\n    def forward(self, x):\n        return super().forward(x)\n        \n\nclass BRLayer(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        \n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass HarDBlock_v2(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n\n    def get_out_ch(self):\n        return self.out_channels\n\n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False, list_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        self.list_out = list_out\n        layers_ = []\n        self.out_channels = 0\n\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          #layers_.append(CatConv2d(inch, outch, (3,3), relu=True))\n          layers_.append(nn.Conv2d(inch, outch, (3,3), relu=True))\n\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n    def transform(self, blk):\n        for i in range(len(self.layers)):\n            self.layers[i].weight[:,:,:,:] = blk.layers[i][0].weight[:,:,:,:]\n            self.layers[i].bias[:] = blk.layers[i][0].bias[:]\n\n    def forward(self, x):\n        layers_ = [x]\n        #self.res = []\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n\n            out = self.layers[layer](tin)\n            #self.res.append(out)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        if self.list_out:\n            return out_\n        else:\n            return torch.cat(out_, 1)\n\n\n\nclass HarDBlock(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n \n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        layers_ = []\n        self.out_channels = 0 # if upsample else in_channels\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          layers_.append(ConvLayer(inch, outch))\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        #print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n\n    def forward(self, x):\n        layers_ = [x]\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n            if len(tin) > 1:\n                x = torch.cat(tin, 1)\n            else:\n                x = tin[0]\n            out = self.layers[layer](x)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        out = torch.cat(out_, 1)\n        return out\n\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #print(\"upsample\",in_channels, out_channels)\n\n    def forward(self, x, skip, concat=True):\n        is_v2 = type(skip) is list\n        if is_v2:\n            skip_x = skip[0]\n        else:\n            skip_x = skip\n        out = F.interpolate(\n                x,\n                size=(skip_x.size(2), skip_x.size(3)),\n                mode=\"bilinear\",\n                align_corners=True,\n                            )\n        if concat:       \n          if is_v2:\n            out = [out] + skip\n          else:                     \n            out = torch.cat([out, skip], 1)\n          \n        return out\n\nclass hardnet(nn.Module):\n    def __init__(self, n_classes=11):\n        super(hardnet, self).__init__()\n\n        first_ch  = [16,24,32,48]\n        ch_list = [  64, 96, 160, 224, 320]\n        grmul = 1.7\n        gr       = [  10,16,18,24,32]\n        n_layers = [   4, 4, 8, 8, 8]\n\n        blks = len(n_layers) \n        self.shortcut_layers = []\n\n        self.base = nn.ModuleList([])\n        self.base.append (\n             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n                       stride=2) )\n        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=3) )\n        self.base.append ( ConvLayer(first_ch[1], first_ch[2],  kernel=3, stride=2) )\n        self.base.append ( ConvLayer(first_ch[2], first_ch[3],  kernel=3) )\n\n        skip_connection_channel_counts = []\n        ch = first_ch[3]\n        for i in range(blks):\n            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n            ch = blk.get_out_ch()\n            skip_connection_channel_counts.append(ch)\n            self.base.append ( blk )\n            if i < blks-1:\n              self.shortcut_layers.append(len(self.base)-1)\n\n            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n            ch = ch_list[i]\n            \n            if i < blks-1:            \n              self.base.append ( nn.AvgPool2d(kernel_size=2, stride=2) )\n\n\n        cur_channels_count = ch\n        prev_block_channels = ch\n        n_blocks = blks-1\n        self.n_blocks =  n_blocks\n\n        # upsampling\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        self.conv1x1_up    = nn.ModuleList([])\n        \n        for i in range(n_blocks-1,-1,-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count//2, kernel=1))\n            cur_channels_count = cur_channels_count//2\n\n            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n            \n            self.denseBlocksUp.append(blk)\n            prev_block_channels = blk.get_out_ch()\n            cur_channels_count = prev_block_channels\n\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n               out_channels=n_classes, kernel_size=1, stride=1,\n               padding=0, bias=True)\n    \n    def v2_transform(self):        \n        for i in range( len(self.base)):\n            if isinstance(self.base[i], HarDBlock):\n                blk = self.base[i]\n                self.base[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=True)\n                self.base[i].transform(blk)\n            elif isinstance(self.base[i], nn.Sequential):\n                blk = self.base[i]\n                sz = blk[0].weight.shape\n                if sz[2] == 1:\n                    #self.base[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                    self.base[i].bias[:] = blk[0].bias[:]\n\n        for i in range(self.n_blocks):\n            blk = self.denseBlocksUp[i]\n            self.denseBlocksUp[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=False)\n            self.denseBlocksUp[i].transform(blk)\n  \n        for i in range(len(self.conv1x1_up)):\n            blk = self.conv1x1_up[i]\n            sz = blk[0].weight.shape\n            if sz[2] == 1:\n                #self.conv1x1_up[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                self.conv1x1_up[i].bias[:] = blk[0].bias[:]                 \n\n    def forward(self, x):\n        \n        skip_connections = []\n        size_in = x.size()\n        \n        \n        for i in range(len(self.base)):\n            x = self.base[i](x)\n            if i in self.shortcut_layers:\n                skip_connections.append(x)\n        out = x\n        \n        for i in range(self.n_blocks):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip, True)\n            out = self.conv1x1_up[i](out)\n            out = self.denseBlocksUp[i](out)\n        \n        out = self.finalConv(out)\n        \n        out = F.interpolate(\n                            out,\n                            size=(size_in[2], size_in[3]),\n                            mode=\"bilinear\",\n                            align_corners=True)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:52:48.413818Z","iopub.execute_input":"2022-05-24T10:52:48.414005Z","iopub.status.idle":"2022-05-24T10:52:48.478106Z","shell.execute_reply.started":"2022-05-24T10:52:48.413982Z","shell.execute_reply":"2022-05-24T10:52:48.477371Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class runningScore(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n        hist = np.bincount(\n            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n        ).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n\n    def get_scores(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return (\n            {\n                \"Overall Acc: \\t\": acc,\n                \"Mean Acc : \\t\": acc_cls,\n                \"FreqW Acc : \\t\": fwavacc,\n                \"Mean IoU : \\t\": mean_iu,\n            },\n            cls_iu,\n        )\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\n\nclass averageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:52:48.479929Z","iopub.execute_input":"2022-05-24T10:52:48.480647Z","iopub.status.idle":"2022-05-24T10:52:48.500033Z","shell.execute_reply.started":"2022-05-24T10:52:48.480596Z","shell.execute_reply":"2022-05-24T10:52:48.499189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n\n\n# Setup seeds\n# torch.manual_seed(1337)\n# torch.cuda.manual_seed(1337)\n# np.random.seed(1337)\n# random.seed(1337)\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Setup Augmentations\ndata_aug = Compose([RandomHorizontallyFlip(0.5), \n                    RandomScaleCrop((448, 448))\n                    ])\n\n# Setup Dataloader\n\ndata_loader = camvidLoader\ndata_path = '../input/camvid/CamVid'\n\nt_loader = data_loader(\n    data_path,\n    is_transform=True,\n    split='train',\n    img_size=(720, 960),\n    augmentations=data_aug,\n)\n\nn_classes = t_loader.n_classes\ntrainloader = data.DataLoader(\n    t_loader,\n    batch_size=16,\n    shuffle=True,\n)\n\n\n\n# Setup Metrics\nrunning_metrics_val = runningScore(n_classes)\n\n# Setup Model\n\nmodel = hardnet()\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint( 'Parameters:',total_params )\n\nmodel = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\nmodel.apply(weights_init)\noptimizer_cls = torch.optim.Adam\n\noptimizer = optimizer_cls(model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=2.5e-5)\nprint(\"Using optimizer {}\".format(optimizer))\n\nnum_epochs = 400\nper_epoch = len(trainloader)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=400, eta_min=1e-6)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=11)\nprint(\"Using loss {}\".format(loss_fn))\n\nstart_epoch = 0\nresume = True\n# file_checkpoint = '../input/models-hardnet-camvid/hardnet_CamVid_checkpoint-final.pkl'\nfile_checkpoint = './hardnet_CamVid_checkpoint.pkl'\nif resume is not None:\n    if os.path.isfile(file_checkpoint):\n        print(\"Loading model and optimizer from checkpoint '{}'\".format(file_checkpoint))\n              \n        checkpoint = torch.load(file_checkpoint)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n        start_epoch = checkpoint[\"epoch\"] + 1\n\n    else:\n        print(\"No checkpoint found at '{}'\".format(file_checkpoint))\n\nval_loss_meter = averageMeter()\ntime_meter = averageMeter()\n\nbest_iou = -100.0\nflag = True\nloss_all = 0\nloss_n = 0\nfor epoch in range(start_epoch, num_epochs):\n    for (images, labels) in trainloader:\n        start_ts = time.time()\n        model.train()\n        images = images.to(device)\n        newlabels = torch.zeros((labels.shape[0], labels.shape[1], labels.shape[2]))\n        for j in range(len(labels)):\n          newlabels[j] = rgb_to_lbl(labels[j, :, :, :], [labels.shape[1], labels.shape[2]])\n        labels = newlabels\n        labels = labels.to(device)\n        labels = labels.long()\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = loss_fn(input=outputs, target=labels)\n        loss.backward()\n        optimizer.step()\n\n        time_meter.update(time.time() - start_ts)\n        loss_all += loss.item()\n        loss_n += 1\n    c_lr = scheduler.get_last_lr()\n    fmt_str = \"Epoch {:d}  Loss: {:.4f}  Time/Image: {:.4f}  lr={:.6f}\"\n    print_str = fmt_str.format(\n        epoch,\n        loss_all / loss_n,\n        time_meter.avg / 16,\n        c_lr[0],\n    )\n\n\n    print(print_str)\n    time_meter.reset()\n    scheduler.step()\n    torch.cuda.empty_cache()\n    loss_all = 0\n    loss_n = 0\n    state = {\n          \"epoch\": epoch,\n          \"model_state\": model.state_dict(),\n          \"optimizer_state\": optimizer.state_dict(),\n          \"scheduler_state\": scheduler.state_dict(),\n    }\n    save_path = os.path.join(\n        './',\n        \"{}_{}_checkpoint.pkl\".format('hardnet', 'CamVid'),\n    )\n    torch.save(state, save_path)\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:52:48.501434Z","iopub.execute_input":"2022-05-24T10:52:48.501888Z","iopub.status.idle":"2022-05-24T11:06:52.854792Z","shell.execute_reply.started":"2022-05-24T10:52:48.501850Z","shell.execute_reply":"2022-05-24T11:06:52.853923Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Parameters: 4118865\nUsing optimizer Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    weight_decay: 2.5e-05\n)\nUsing loss CrossEntropyLoss()\nLoading model and optimizer from checkpoint './hardnet_CamVid_checkpoint.pkl'\nEpoch 380  Loss: 0.1274  Time/Image: 0.0214  lr=0.000007\nEpoch 381  Loss: 0.1211  Time/Image: 0.0210  lr=0.000007\nEpoch 382  Loss: 0.1280  Time/Image: 0.0217  lr=0.000006\nEpoch 383  Loss: 0.1243  Time/Image: 0.0219  lr=0.000005\nEpoch 384  Loss: 0.1246  Time/Image: 0.0216  lr=0.000005\nEpoch 385  Loss: 0.1280  Time/Image: 0.0209  lr=0.000004\nEpoch 386  Loss: 0.1215  Time/Image: 0.0213  lr=0.000004\nEpoch 387  Loss: 0.1232  Time/Image: 0.0215  lr=0.000004\nEpoch 388  Loss: 0.1231  Time/Image: 0.0216  lr=0.000003\nEpoch 389  Loss: 0.1246  Time/Image: 0.0210  lr=0.000003\nEpoch 390  Loss: 0.1272  Time/Image: 0.0210  lr=0.000003\nEpoch 391  Loss: 0.1291  Time/Image: 0.0214  lr=0.000002\nEpoch 392  Loss: 0.1284  Time/Image: 0.0215  lr=0.000002\nEpoch 393  Loss: 0.1251  Time/Image: 0.0210  lr=0.000002\nEpoch 394  Loss: 0.1259  Time/Image: 0.0217  lr=0.000002\nEpoch 395  Loss: 0.1253  Time/Image: 0.0214  lr=0.000001\nEpoch 396  Loss: 0.1299  Time/Image: 0.0209  lr=0.000001\nEpoch 397  Loss: 0.1246  Time/Image: 0.0213  lr=0.000001\nEpoch 398  Loss: 0.1232  Time/Image: 0.0216  lr=0.000001\nEpoch 399  Loss: 0.1313  Time/Image: 0.0217  lr=0.000001\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nte_loader = data_loader(\n    data_path,\n    split=\"test\",\n    is_transform=True,\n    img_size=(720, 960)\n)\ntestloader = data.DataLoader(\n    te_loader,\n    batch_size=16,\n    shuffle=True)\n\nbest = torch.load('./hardnet_CamVid_checkpoint.pkl')\nmodel.load_state_dict(best[\"model_state\"])\n\nrunning_score_test = runningScore(n_classes)\nmodel.eval()\nwith torch.no_grad():\n    for i, data_samples in tqdm(enumerate(testloader)):\n        imgs_test, labels_test = data_samples\n        # print(imgs.size())\n#         print(labels_test.shape)\n        outputs = model(imgs_test).argmax(axis=1)\n        imgs_test = imgs_test.numpy()[:, ::-1, :, :]\n        imgs_test = np.transpose(imgs_test, [0, 2, 3, 1])\n        # print(outputs.dtype)\n        newlabels_test=torch.zeros((labels_test.shape[0], labels_test.shape[1] , labels_test.shape[2]))\n        # print(type(labels))\n        for j in range(len(labels_test)):\n          #print(labels[i,:,:,:].shape)\n          # print(labels[i, :, :, :].size(), imgs.shape)\n          newlabels_test[j] = rgb_to_lbl(labels_test[j, :, :, :], [labels_test.shape[1], labels_test.shape[2]])\n\n        labels_test = newlabels_test.type(dtype=torch.int64)\n#         print(labels_test[labels_test == 4].shape)\n#         print(torch.max(outputs), torch.max(labels_test))\n        running_score_test.update(labels_test.cpu().numpy(), outputs.cpu().numpy())\n    score, class_iou = running_score_test.get_scores()\n    label_to_class = {\n                      0: 'nebo', \n                      1: 'zgrada', \n                      2: 'stup',\n                      3: 'cesta',\n                      4: 'pločnik',\n                      5: 'drvo',\n                      6: 'prometni znak/simbol',\n                      7: 'ograda',\n                      8: 'auto',\n                      9: 'pješak',\n                      10: 'biciklist',\n                      11: 'neoznačeno'\n                     }\n    for k, v in class_iou.items():\n        print(label_to_class[k], v)\n    for k, v in score.items():\n        print(k, v)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:10:17.228177Z","iopub.execute_input":"2022-05-24T11:10:17.228444Z","iopub.status.idle":"2022-05-24T11:10:52.834298Z","shell.execute_reply.started":"2022-05-24T11:10:17.228402Z","shell.execute_reply":"2022-05-24T11:10:52.833438Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"15it [00:35,  2.36s/it]","output_type":"stream"},{"name":"stdout","text":"nebo 0.9363860471697698\nzgrada 0.9055464470598287\nstup 0.3506198512538707\ncesta 0.965175029859152\npločnik 0.8643719517887314\ndrvo 0.8133867908468055\nprometni znak/simbol 0.5034642646347235\nograda 0.644952570381572\nauto 0.8995928817154749\npješak 0.5485561222617693\nbiciklist 0.6301018980028799\nOverall Acc: \t 0.9451776249813325\nMean Acc : \t 0.80441986097375\nFreqW Acc : \t 0.8995795585288185\nMean IoU : \t 0.7329230777249617\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}