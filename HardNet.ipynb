{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport numbers\nimport random\nimport torchvision.transforms.functional as tf\n\nfrom PIL import Image, ImageOps\n\n\nclass Compose(object):\n    def __init__(self, augmentations):\n        self.augmentations = augmentations\n        self.PIL2Numpy = False\n\n    def __call__(self, img, mask):\n        if isinstance(img, np.ndarray):\n            img = Image.fromarray(img, mode=\"RGB\")\n            mask = Image.fromarray(mask, mode=\"RGB\")\n            self.PIL2Numpy = True\n\n        assert img.size == mask.size\n        for a in self.augmentations:\n            img, mask = a(img, mask)\n\n        if self.PIL2Numpy:\n            img, mask = np.array(img), np.array(mask, dtype=np.uint8)\n\n        return img, mask\n\nclass RandomHorizontallyFlip(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            return (img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT))\n        return img, mask\n        \nclass RandomScaleCrop(object):\n    def __init__(self, size):\n        self.size = size\n        self.crop = RandomCrop(self.size)\n            \n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        r = random.uniform(0.5, 2.0)\n        w, h = img.size\n        new_size = (int(w*r),int(h*r))\n        return self.crop(img.resize(new_size, Image.BILINEAR), mask.resize(new_size, Image.NEAREST))\n    \nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        ch, cw = self.size\n        if w == cw and h == ch:\n            return img, mask\n        if w < cw or h < ch:\n            pw = cw - w if cw > w else 0\n            ph = ch - h if ch > h else 0\n            padding = (pw,ph,pw,ph)\n            img  = ImageOps.expand(img,  padding, fill=0)\n            mask = ImageOps.expand(mask, padding, fill=250)\n            w, h = img.size\n            assert img.size == mask.size\n            \n        x1 = random.randint(0, w - cw)\n        y1 = random.randint(0, h - ch)\n        return (img.crop((x1, y1, x1 + cw, y1 + ch)), mask.crop((x1, y1, x1 + cw, y1 + ch)))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-25T10:47:59.096885Z","iopub.execute_input":"2022-05-25T10:47:59.097656Z","iopub.status.idle":"2022-05-25T10:47:59.117481Z","shell.execute_reply.started":"2022-05-25T10:47:59.097615Z","shell.execute_reply":"2022-05-25T10:47:59.116749Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"from traitlets.traitlets import Long\n#data loader\nimport collections\nimport torch\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nimport skimage.transform\nfrom torch.utils import data\n\n\ndef rgb_to_lbl(rgb, size):\n    Sky = [128, 128, 128]\n    Building = [128, 0, 0]\n    Pole = [192, 192, 128]\n    Road = [128, 64, 128]\n    Pavement = [0, 0, 192]\n    Tree = [128, 128, 0]\n    SignSymbol = [192, 128, 128]\n    Fence = [64, 64, 128]\n    Car = [64, 0, 128]\n    Pedestrian = [64, 64, 0]\n    Bicyclist = [0, 128, 192]\n#     Unlabelled = [0, 0, 0]\n\n    label_colours = np.array(\n        [\n            Sky,\n            Building,\n            Pole,\n            Road,\n            Pavement,\n            Tree,\n            SignSymbol,\n            Fence,\n            Car,\n            Pedestrian,\n            Bicyclist,\n#             Unlabelled,\n        ]\n    )\n    label = 11 * torch.ones(size[0] * size[1], dtype=torch.uint8)\n    w, h, t = rgb.shape\n    rgb = rgb.reshape(-1, t)\n    for l in range(0, len(label_colours)):\n      r = rgb[:, 0] == label_colours[l, 0]\n      g = rgb[:, 1] == label_colours[l, 1]\n      b = rgb[:, 2] == label_colours[l, 2]\n      tf_rgb = r * g * b\n\n      label[tf_rgb] = l\n    label = label.reshape(size[0], size[1])\n    #print(rgb2[360][260:270], label[360][260:270])\n    return label\n\nclass camvidLoader(data.Dataset):\n    def __init__(\n        self,\n        root,\n        split=\"train\",\n        is_transform=False,\n        img_size=None,\n        augmentations=None,\n        img_norm=True,\n        test_mode=False\n    ):\n        self.root = root\n        self.split = split\n        self.img_size = img_size\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n        self.img_norm = img_norm\n        self.test_mode = test_mode\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 11\n        self.files = collections.defaultdict(list)\n\n        if not self.test_mode:\n            for split in [\"train\", \"test\", \"val\"]:\n                file_list = os.listdir(root + \"/\" + split)\n                self.files[split] = file_list\n            self.files['train'] += self.files['val'][:]\n            del self.files['val']\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        img_name = self.files[self.split][index]\n        if os.path.exists(self.root + \"/\" + self.split + \"/\" + img_name):\n            img_path = self.root + \"/\" + self.split + \"/\" + img_name\n            lbl_path = self.root + \"/\" + self.split + \"_labels/\" + img_name.split('.')[0] + '_L.' + img_name.split('.')[1]\n        else:\n            img_path = self.root + \"/\" + 'val' + \"/\" + img_name\n            lbl_path = self.root + \"/\" + 'val' + \"_labels/\" + img_name.split('.')[0] + '_L.' + img_name.split('.')[1]\n        img = plt.imread(img_path)\n        img = np.array(img*255, dtype=np.uint8)\n\n        lbl = plt.imread(lbl_path)\n        lbl = np.array(lbl*255, dtype=np.uint8)\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n            \n        lbl = rgb_to_lbl(lbl, (lbl.shape[0], lbl.shape[1], lbl.shape[2]))\n        return img, lbl\n\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]  # RGB -> BGR\n        img = img.astype(np.float64)\n        img -= self.mean\n        if self.img_norm:\n            # Resize scales images from 0 to 255, thus we need\n            # to divide by 255.0\n            img = img.astype(float) / 255.0\n        # NHWC -> NCHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        Sky = [128, 128, 128]\n        Building = [128, 0, 0]\n        Pole = [192, 192, 128]\n        Road = [128, 64, 128]\n        Pavement = [0, 0, 192]\n        Tree = [128, 128, 0]\n        SignSymbol = [192, 128, 128]\n        Fence = [64, 64, 128]\n        Car = [64, 0, 128]\n        Pedestrian = [64, 64, 0]\n        Bicyclist = [0, 128, 192]\n        # Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Sky,\n                Building,\n                Pole,\n                Road,\n                Pavement,\n                Tree,\n                SignSymbol,\n                Fence,\n                Car,\n                Pedestrian,\n                Bicyclist,\n                # Unlabelled,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n        rgb[:, :, 0] = r / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = b / 255.0\n        return rgb","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:47:59.157122Z","iopub.execute_input":"2022-05-25T10:47:59.157369Z","iopub.status.idle":"2022-05-25T10:47:59.188668Z","shell.execute_reply.started":"2022-05-25T10:47:59.157340Z","shell.execute_reply":"2022-05-25T10:47:59.187775Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"# model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\n\nclass ConvLayer(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1):\n        super().__init__()\n        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n                                          stride=stride, padding=kernel//2, bias = False))\n        self.add_module('norm', nn.BatchNorm2d(out_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n\n        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n\n    def forward(self, x):\n        return super().forward(x)\n        \n\nclass BRLayer(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        \n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass HarDBlock_v2(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n\n    def get_out_ch(self):\n        return self.out_channels\n\n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False, list_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        self.list_out = list_out\n        layers_ = []\n        self.out_channels = 0\n\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          #layers_.append(CatConv2d(inch, outch, (3,3), relu=True))\n          layers_.append(nn.Conv2d(inch, outch, (3,3), relu=True))\n\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        #print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n    def transform(self, blk):\n        for i in range(len(self.layers)):\n            self.layers[i].weight[:,:,:,:] = blk.layers[i][0].weight[:,:,:,:]\n            self.layers[i].bias[:] = blk.layers[i][0].bias[:]\n\n    def forward(self, x):\n        layers_ = [x]\n        #self.res = []\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n\n            out = self.layers[layer](tin)\n            #self.res.append(out)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        if self.list_out:\n            return out_\n        else:\n            return torch.cat(out_, 1)\n\n\n\nclass HarDBlock(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n \n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        layers_ = []\n        self.out_channels = 0 # if upsample else in_channels\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          layers_.append(ConvLayer(inch, outch))\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        #print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n\n    def forward(self, x):\n        layers_ = [x]\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n            if len(tin) > 1:\n                x = torch.cat(tin, 1)\n            else:\n                x = tin[0]\n            out = self.layers[layer](x)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        out = torch.cat(out_, 1)\n        return out\n\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #print(\"upsample\",in_channels, out_channels)\n\n    def forward(self, x, skip, concat=True):\n        is_v2 = type(skip) is list\n        if is_v2:\n            skip_x = skip[0]\n        else:\n            skip_x = skip\n        out = F.interpolate(\n                x,\n                size=(skip_x.size(2), skip_x.size(3)),\n                mode=\"bilinear\",\n                align_corners=True,\n                            )\n        if concat:       \n          if is_v2:\n            out = [out] + skip\n          else:                     \n            out = torch.cat([out, skip], 1)\n          \n        return out\n\nclass hardnet(nn.Module):\n    def __init__(self, n_classes=11):\n        super(hardnet, self).__init__()\n\n        first_ch  = [16,24,32,48]\n        ch_list = [  64, 96, 160, 224, 320]\n        grmul = 1.7\n        gr       = [  10,16,18,24,32]\n        n_layers = [   4, 4, 8, 8, 8]\n\n        blks = len(n_layers) \n        self.shortcut_layers = []\n\n        self.base = nn.ModuleList([])\n        self.base.append (\n             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n                       stride=2) )\n        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=3) )\n        self.base.append ( ConvLayer(first_ch[1], first_ch[2],  kernel=3, stride=2) )\n        self.base.append ( ConvLayer(first_ch[2], first_ch[3],  kernel=3) )\n\n        skip_connection_channel_counts = []\n        ch = first_ch[3]\n        for i in range(blks):\n            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n            ch = blk.get_out_ch()\n            skip_connection_channel_counts.append(ch)\n            self.base.append ( blk )\n            if i < blks-1:\n              self.shortcut_layers.append(len(self.base)-1)\n\n            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n            ch = ch_list[i]\n            \n            if i < blks-1:            \n              self.base.append ( nn.AvgPool2d(kernel_size=2, stride=2) )\n\n\n        cur_channels_count = ch\n        prev_block_channels = ch\n        n_blocks = blks-1\n        self.n_blocks =  n_blocks\n\n        # upsampling\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        self.conv1x1_up    = nn.ModuleList([])\n        \n        for i in range(n_blocks-1,-1,-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count//2, kernel=1))\n            cur_channels_count = cur_channels_count//2\n\n            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n            \n            self.denseBlocksUp.append(blk)\n            prev_block_channels = blk.get_out_ch()\n            cur_channels_count = prev_block_channels\n\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n               out_channels=n_classes, kernel_size=1, stride=1,\n               padding=0, bias=True)\n    \n    def v2_transform(self):        \n        for i in range( len(self.base)):\n            if isinstance(self.base[i], HarDBlock):\n                blk = self.base[i]\n                self.base[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=True)\n                self.base[i].transform(blk)\n            elif isinstance(self.base[i], nn.Sequential):\n                blk = self.base[i]\n                sz = blk[0].weight.shape\n                if sz[2] == 1:\n                    #self.base[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                    self.base[i].bias[:] = blk[0].bias[:]\n\n        for i in range(self.n_blocks):\n            blk = self.denseBlocksUp[i]\n            self.denseBlocksUp[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=False)\n            self.denseBlocksUp[i].transform(blk)\n  \n        for i in range(len(self.conv1x1_up)):\n            blk = self.conv1x1_up[i]\n            sz = blk[0].weight.shape\n            if sz[2] == 1:\n                #self.conv1x1_up[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                self.conv1x1_up[i].bias[:] = blk[0].bias[:]                 \n\n    def forward(self, x):\n        \n        skip_connections = []\n        size_in = x.size()\n        \n        \n        for i in range(len(self.base)):\n            x = self.base[i](x)\n            if i in self.shortcut_layers:\n                skip_connections.append(x)\n        out = x\n        \n        for i in range(self.n_blocks):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip, True)\n            out = self.conv1x1_up[i](out)\n            out = self.denseBlocksUp[i](out)\n        \n        out = self.finalConv(out)\n        \n        out = F.interpolate(\n                            out,\n                            size=(size_in[2], size_in[3]),\n                            mode=\"bilinear\",\n                            align_corners=True)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:47:59.217740Z","iopub.execute_input":"2022-05-25T10:47:59.218174Z","iopub.status.idle":"2022-05-25T10:47:59.280385Z","shell.execute_reply.started":"2022-05-25T10:47:59.218142Z","shell.execute_reply":"2022-05-25T10:47:59.279550Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"class runningScore(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n        hist = np.bincount(\n            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n        ).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n\n    def get_scores(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return (\n            {\n                \"Overall Acc: \\t\": acc,\n                \"Mean Acc : \\t\": acc_cls,\n                \"Mean IoU : \\t\": mean_iu,\n            },\n            cls_iu,\n        )\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\n\nclass averageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:47:59.282037Z","iopub.execute_input":"2022-05-25T10:47:59.282523Z","iopub.status.idle":"2022-05-25T10:47:59.298502Z","shell.execute_reply.started":"2022-05-25T10:47:59.282466Z","shell.execute_reply":"2022-05-25T10:47:59.297762Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n\n\n# Setup seeds\n# torch.manual_seed(1337)\n# torch.cuda.manual_seed(1337)\n# np.random.seed(1337)\n# random.seed(1337)\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Setup Augmentations\ndata_aug = Compose([RandomHorizontallyFlip(0.5), \n                    RandomScaleCrop((448, 448))\n                    ])\n\n# Setup Dataloader\n\ndata_loader = camvidLoader\ndata_path = '../input/camvid/CamVid'\n\nt_loader = data_loader(\n    data_path,\n    is_transform=True,\n    split='train',\n    img_size=(720, 960),\n    augmentations=data_aug,\n)\n\nn_classes = t_loader.n_classes\ntrainloader = data.DataLoader(\n    t_loader,\n    batch_size=16,\n    shuffle=True,\n)\n\n\n\n# Setup Metrics\nrunning_metrics_val = runningScore(n_classes)\n\n# Setup Model\n\nmodel = hardnet()\n\ntotal_params = sum(p.numel() for p in model.parameters())\n# print( 'Parameters:',total_params )\n\nmodel = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\nmodel.apply(weights_init)\noptimizer_cls = torch.optim.Adam\n\noptimizer = optimizer_cls(model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=2.5e-5)\n# print(\"Using optimizer {}\".format(optimizer))\n\nnum_epochs = 400\nper_epoch = len(trainloader)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=400, eta_min=1e-6)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=11)\n# print(\"Using loss {}\".format(loss_fn))\n\nstart_epoch = 0\nresume = None\n# file_checkpoint = '../input/models-hardnet-camvid/hardnet_CamVid_checkpoint-final.pkl'\nfile_checkpoint = './hardnet_CamVid_checkpoint.pkl'\nif resume is not None:\n    if os.path.isfile(file_checkpoint):\n#         print(\"Loading model and optimizer from checkpoint '{}'\".format(file_checkpoint))\n              \n        checkpoint = torch.load(file_checkpoint)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n        start_epoch = checkpoint[\"epoch\"] + 1\n\n    else:\n        print(\"No checkpoint found at '{}'\".format(file_checkpoint))\n        \n    time_meter = averageMeter()\n    loss_all = 0\n    loss_n = 0\n    for epoch in range(start_epoch, num_epochs):\n        for (images, labels) in trainloader:\n            start_ts = time.time()\n            model.train()\n            images = images.to(device)\n\n            labels = labels.to(device)\n            labels = labels.long()\n\n            optimizer.zero_grad()\n            outputs = model(images)\n\n            loss = loss_fn(input=outputs, target=labels)\n            loss.backward()\n            optimizer.step()\n\n            time_meter.update(time.time() - start_ts)\n            loss_all += loss.item()\n            loss_n += 1\n        c_lr = scheduler.get_last_lr()\n        fmt_str = \"Epoch {:d}  Loss: {:.4f}  Time/Image: {:.4f}  lr={:.6f}\"\n        print_str = fmt_str.format(\n            epoch,\n            loss_all / loss_n,\n            time_meter.avg / 16,\n            c_lr[0],\n        )\n\n\n        print(print_str)\n        time_meter.reset()\n        scheduler.step()\n        torch.cuda.empty_cache()\n        loss_all = 0\n        loss_n = 0\n        state = {\n              \"epoch\": epoch,\n              \"model_state\": model.state_dict(),\n              \"optimizer_state\": optimizer.state_dict(),\n              \"scheduler_state\": scheduler.state_dict(),\n        }\n        save_path = os.path.join(\n            './',\n            \"{}_{}_checkpoint.pkl\".format('hardnet', 'CamVid'),\n        )\n        torch.save(state, save_path)\n        torch.cuda.empty_cache()\n    save_path = \"./trained_model.pth\"\n    torch.save(model, save_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:47:59.392344Z","iopub.execute_input":"2022-05-25T10:47:59.392608Z","iopub.status.idle":"2022-05-25T10:47:59.494599Z","shell.execute_reply.started":"2022-05-25T10:47:59.392578Z","shell.execute_reply":"2022-05-25T10:47:59.493880Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom time import perf_counter\n\nte_loader = data_loader(\n    data_path,\n    split=\"test\",\n    is_transform=True,\n    img_size=(720, 960)\n)\ntestloader = data.DataLoader(\n    te_loader,\n    batch_size=1,\n)\nn = len(testloader)\nmodel = torch.load(\"../input/models-hardnet-camvid/hardnet_petite_base.pth\")\n\nrunning_score_test = runningScore(n_classes)\nmodel.eval()\nmodel.to(device)\n\nwith torch.no_grad():\n    torch.cuda.synchronize()\n    t0 = perf_counter()\n    \n    for i, data_samples in tqdm(enumerate(testloader)):\n        imgs_test, labels_test = data_samples\n        outputs = model(imgs_test)\n        imgs_test = imgs_test.numpy()[:, ::-1, :, :]\n        imgs_test = np.transpose(imgs_test, [0, 2, 3, 1])\n        \n        pred = outputs.data.max(1)[1].cpu().numpy()\n        #print(pred.shape)\n        gt = labels_test.numpy()\n        \n        running_score_test.update(gt, pred)\n        \n    torch.cuda.synchronize()\n    t1 = perf_counter()\n    fps = n / (t1 - t0)\n    score, class_iou = running_score_test.get_scores()\n    label_to_class = {\n                      0: 'nebo', \n                      1: 'zgrada', \n                      2: 'stup',\n                      3: 'cesta',\n                      4: 'pločnik',\n                      5: 'drvo',\n                      6: 'prometni znak/simbol',\n                      7: 'ograda',\n                      8: 'auto',\n                      9: 'pješak',\n                      10: 'biciklist',\n                      11: 'neoznačeno'\n                     }\n    for k, v in score.items():\n        print(k, v)\n    for k, v in class_iou.items():\n        print(label_to_class[k], v)\n    print('FPS: {}'.format(fps) )","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:47:59.499260Z","iopub.execute_input":"2022-05-25T10:47:59.499457Z","iopub.status.idle":"2022-05-25T10:48:31.394987Z","shell.execute_reply.started":"2022-05-25T10:47:59.499433Z","shell.execute_reply":"2022-05-25T10:48:31.394227Z"},"trusted":true},"execution_count":192,"outputs":[{"name":"stderr","text":"232it [00:31,  7.30it/s]","output_type":"stream"},{"name":"stdout","text":"Overall Acc: \t 0.9427635139586239\nMean Acc : \t 0.8017656451136308\nMean IoU : \t 0.7280773713819635\nnebo 0.9361529753900554\nzgrada 0.8988513039341394\nstup 0.3468522716068099\ncesta 0.9635776536744478\npločnik 0.8606682443700884\ndrvo 0.8030446427570029\nprometni znak/simbol 0.48458443598184126\nograda 0.6438961441757947\nauto 0.8928354201577365\npješak 0.5565552625831034\nbiciklist 0.6218327305705785\nFPS: 7.294681398000166\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}