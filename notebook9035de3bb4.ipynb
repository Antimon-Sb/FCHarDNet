{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numbers\nimport random\nimport torchvision.transforms.functional as tf\n\nfrom PIL import Image, ImageOps\n\n\nclass Compose(object):\n    def __init__(self, augmentations):\n        self.augmentations = augmentations\n        self.PIL2Numpy = False\n\n    def __call__(self, img, mask):\n        if isinstance(img, np.ndarray):\n            img = Image.fromarray(img, mode=\"RGB\")\n            mask = Image.fromarray(mask, mode=\"RGB\")\n            self.PIL2Numpy = True\n\n        assert img.size == mask.size\n        for a in self.augmentations:\n            img, mask = a(img, mask)\n\n        if self.PIL2Numpy:\n            img, mask = np.array(img), np.array(mask, dtype=np.uint8)\n\n        return img, mask\n\nclass RandomHorizontallyFlip(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            return (img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT))\n        return img, mask\n        \nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img, mask):\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            mask = ImageOps.expand(mask, border=self.padding, fill=0)\n\n        assert img.size == mask.size\n        w, h = img.size\n        ch, cw = self.size\n        if w == cw and h == ch:\n            return img, mask\n        if w < cw or h < ch:\n            pw = cw - w if cw > w else 0\n            ph = ch - h if ch > h else 0\n            padding = (pw,ph,pw,ph)\n            img  = ImageOps.expand(img,  padding, fill=0)\n            mask = ImageOps.expand(mask, padding, fill=250)\n            w, h = img.size\n            assert img.size == mask.size\n            \n        x1 = random.randint(0, w - cw)\n        y1 = random.randint(0, h - ch)\n        return (img.crop((x1, y1, x1 + cw, y1 + ch)), mask.crop((x1, y1, x1 + cw, y1 + ch)))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T19:55:01.178926Z","iopub.execute_input":"2022-05-04T19:55:01.179545Z","iopub.status.idle":"2022-05-04T19:55:02.984649Z","shell.execute_reply.started":"2022-05-04T19:55:01.179446Z","shell.execute_reply":"2022-05-04T19:55:02.983564Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from traitlets.traitlets import Long\n#data loader\nimport collections\nimport torch\nimport numpy as np\nimport scipy.misc as m\nimport matplotlib.pyplot as plt\nimport skimage.transform\nfrom torch.utils import data\n\n\ndef rgb_to_lbl(rgb):\n    Sky = [128, 128, 128]\n    Building = [128, 0, 0]\n    Pole = [192, 192, 128]\n    Road = [128, 64, 128]\n    Pavement = [60, 40, 222]\n    Tree = [128, 128, 0]\n    SignSymbol = [192, 128, 128]\n    Fence = [64, 64, 128]\n    Car = [64, 0, 128]\n    Pedestrian = [64, 64, 0]\n    Bicyclist = [0, 128, 192]\n#     Unlabelled = [0, 0, 0]\n\n    label_colours = np.array(\n        [\n            Sky,\n            Building,\n            Pole,\n            Road,\n            Pavement,\n            Tree,\n            SignSymbol,\n            Fence,\n            Car,\n            Pedestrian,\n            Bicyclist,\n#             Unlabelled,\n        ]\n    )\n    label = 11 * torch.ones(960 * 720, dtype=torch.uint8)\n    w, h, t = rgb.size()\n    rgb2 = rgb[:]\n    rgb = rgb.view(-1, t)\n    for l in range(0, len(label_colours)):\n      r = rgb[:, 0] == label_colours[l, 0]\n      g = rgb[:, 1] == label_colours[l, 1]\n      b = rgb[:, 2] == label_colours[l, 2]\n      tf_rgb = r * g * b\n      label[tf_rgb] = l\n    label = label.reshape(960, 720)\n    #print(rgb2[360][260:270], label[360][260:270])\n    return label\n\nclass camvidLoader(data.Dataset):\n    def __init__(\n        self,\n        root,\n        split=\"train\",\n        is_transform=False,\n        img_size=None,\n        augmentations=None,\n        img_norm=True,\n        test_mode=False,\n    ):\n        self.root = root\n        self.split = split\n        self.img_size = [960, 720]\n        self.is_transform = is_transform\n        self.augmentations = augmentations\n        self.img_norm = img_norm\n        self.test_mode = test_mode\n        self.mean = np.array([104.00699, 116.66877, 122.67892])\n        self.n_classes = 12\n        self.files = collections.defaultdict(list)\n\n        if not self.test_mode:\n            for split in [\"train\", \"test\", \"val\"]:\n                file_list = os.listdir(root + \"/\" + split)\n                self.files[split] = file_list\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        img_name = self.files[self.split][index]\n        img_path = self.root + \"/\" + self.split + \"/\" + img_name\n        lbl_path = self.root + \"/\" + self.split + \"_labels/\" + img_name.split('.')[0] + '_L.' + img_name.split('.')[1]\n\n        img = plt.imread(img_path)\n        img = np.array(img*255, dtype=np.uint8)\n\n        lbl = plt.imread(lbl_path)\n        lbl = np.array(lbl*255, dtype=np.uint8)\n\n        if self.augmentations is not None:\n            img, lbl = self.augmentations(img, lbl)\n\n        if self.is_transform:\n            img, lbl = self.transform(img, lbl)\n        return img, lbl\n\n    def transform(self, img, lbl):\n#         print(img.shape, lbl.shape)\n        img = skimage.transform.resize(img, (self.img_size[0], self.img_size[1]), preserve_range=True)  # uint8 with RGB mode\n        lbl = skimage.transform.resize(lbl, (self.img_size[0], self.img_size[1]), anti_aliasing=False, preserve_range=True)\n#         print(img.shape, lbl.shape)\n        img = img[:, :, ::-1]  # RGB -> BGR\n        img = img.astype(np.float64)\n        img -= self.mean\n        if self.img_norm:\n            # Resize scales images from 0 to 255, thus we need\n            # to divide by 255.0\n            img = img.astype(float) / 255.0\n        # NHWC -> NCHW\n        img = img.transpose(2, 0, 1)\n\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def decode_segmap(self, temp, plot=False):\n        Sky = [128, 128, 128]\n        Building = [128, 0, 0]\n        Pole = [192, 192, 128]\n        Road = [128, 64, 128]\n        Pavement = [60, 40, 222]\n        Tree = [128, 128, 0]\n        SignSymbol = [192, 128, 128]\n        Fence = [64, 64, 128]\n        Car = [64, 0, 128]\n        Pedestrian = [64, 64, 0]\n        Bicyclist = [0, 128, 192]\n#         Unlabelled = [0, 0, 0]\n\n        label_colours = np.array(\n            [\n                Sky,\n                Building,\n                Pole,\n                Road,\n                Pavement,\n                Tree,\n                SignSymbol,\n                Fence,\n                Car,\n                Pedestrian,\n                Bicyclist,\n#                 Unlabelled,\n            ]\n        )\n        r = temp.copy()\n        g = temp.copy()\n        b = temp.copy()\n        for l in range(0, self.n_classes):\n            r[temp == l] = label_colours[l, 0]\n            g[temp == l] = label_colours[l, 1]\n            b[temp == l] = label_colours[l, 2]\n\n        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n        rgb[:, :, 0] = r / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = b / 255.0\n        return rgb","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:02.990425Z","iopub.execute_input":"2022-05-04T19:55:02.990779Z","iopub.status.idle":"2022-05-04T19:55:04.058198Z","shell.execute_reply.started":"2022-05-04T19:55:02.990737Z","shell.execute_reply":"2022-05-04T19:55:04.057479Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n# #testing data loader\n# local_path = \"../input/camvid/CamVid\"\n# augmentations = Compose([RandomHorizontallyFlip(0.5), RandomCrop((270, 360))])\n\n# dst = camvidLoader(local_path, is_transform=True, augmentations=augmentations)\n# bs = 4\n# trainloader = data.DataLoader(dst, batch_size=bs, shuffle=True)\n# for i, data_samples in enumerate(trainloader):\n#     imgs, labels = data_samples\n#     # print(imgs.size())\n#     imgs = imgs.numpy()[:, ::-1, :, :]\n#     imgs = np.transpose(imgs, [0, 2, 3, 1])\n#     newlabels=torch.zeros((len(labels), 960, 720))\n#     # print(type(labels))\n#     for i in range(len(labels)):\n#       #print(labels[i,:,:,:].shape)\n#       # print(labels[i, :, :, :].size(), imgs.shape)\n#       newlabels[i] = rgb_to_lbl(labels[i,:,:,:])\n#     labels = newlabels\n#     f, axarr = plt.subplots(bs, 2)\n#     for j in range(bs):\n#         axarr[j][0].imshow(imgs[j])\n#         axarr[j][1].imshow(dst.decode_segmap(newlabels.numpy()[j]))\n#     plt.show()\n#     a = input()\n#     if a == \"ex\":\n#         break\n#     else:\n#         plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:04.059377Z","iopub.execute_input":"2022-05-04T19:55:04.059665Z","iopub.status.idle":"2022-05-04T19:55:04.065362Z","shell.execute_reply.started":"2022-05-04T19:55:04.059606Z","shell.execute_reply":"2022-05-04T19:55:04.064730Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\n#from CatConv2d.catconv2d import CatConv2d\n\nclass ConvLayer(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1):\n        super().__init__()\n        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n                                          stride=stride, padding=kernel//2, bias = False))\n        self.add_module('norm', nn.BatchNorm2d(out_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n\n        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n\n    def forward(self, x):\n        return super().forward(x)\n        \n\nclass BRLayer(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        \n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass HarDBlock_v2(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n\n    def get_out_ch(self):\n        return self.out_channels\n\n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False, list_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        self.list_out = list_out\n        layers_ = []\n        self.out_channels = 0\n\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          #layers_.append(CatConv2d(inch, outch, (3,3), relu=True))\n          layers_.append(nn.Conv2d(inch, outch, (3,3), relu=True))\n\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n    def transform(self, blk):\n        for i in range(len(self.layers)):\n            self.layers[i].weight[:,:,:,:] = blk.layers[i][0].weight[:,:,:,:]\n            self.layers[i].bias[:] = blk.layers[i][0].bias[:]\n\n    def forward(self, x):\n        layers_ = [x]\n        #self.res = []\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n\n            out = self.layers[layer](tin)\n            #self.res.append(out)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        if self.list_out:\n            return out_\n        else:\n            return torch.cat(out_, 1)\n\n\n\nclass HarDBlock(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n \n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        layers_ = []\n        self.out_channels = 0 # if upsample else in_channels\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          layers_.append(ConvLayer(inch, outch))\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        #print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n\n    def forward(self, x):\n        layers_ = [x]\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n            if len(tin) > 1:\n                x = torch.cat(tin, 1)\n            else:\n                x = tin[0]\n            out = self.layers[layer](x)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        out = torch.cat(out_, 1)\n        return out\n\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #print(\"upsample\",in_channels, out_channels)\n\n    def forward(self, x, skip, concat=True):\n        is_v2 = type(skip) is list\n        if is_v2:\n            skip_x = skip[0]\n        else:\n            skip_x = skip\n        out = F.interpolate(\n                x,\n                size=(skip_x.size(2), skip_x.size(3)),\n                mode=\"bilinear\",\n                align_corners=True,\n                            )\n        if concat:       \n          if is_v2:\n            out = [out] + skip\n          else:                     \n            out = torch.cat([out, skip], 1)\n          \n        return out\n\nclass hardnet(nn.Module):\n    def __init__(self, n_classes=11):\n        super(hardnet, self).__init__()\n\n        first_ch  = [16,24,32,48]\n        ch_list = [  64, 96, 160, 224, 320]\n        grmul = 1.7\n        gr       = [  10,16,18,24,32]\n        n_layers = [   4, 4, 8, 8, 8]\n\n        blks = len(n_layers) \n        self.shortcut_layers = []\n\n        self.base = nn.ModuleList([])\n        self.base.append (\n             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n                       stride=2) )\n        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=3) )\n        self.base.append ( ConvLayer(first_ch[1], first_ch[2],  kernel=3, stride=2) )\n        self.base.append ( ConvLayer(first_ch[2], first_ch[3],  kernel=3) )\n\n        skip_connection_channel_counts = []\n        ch = first_ch[3]\n        for i in range(blks):\n            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n            ch = blk.get_out_ch()\n            skip_connection_channel_counts.append(ch)\n            self.base.append ( blk )\n            if i < blks-1:\n              self.shortcut_layers.append(len(self.base)-1)\n\n            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n            ch = ch_list[i]\n            \n            if i < blks-1:            \n              self.base.append ( nn.AvgPool2d(kernel_size=2, stride=2) )\n\n\n        cur_channels_count = ch\n        prev_block_channels = ch\n        n_blocks = blks-1\n        self.n_blocks =  n_blocks\n\n        # upsampling\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        self.conv1x1_up    = nn.ModuleList([])\n        \n        for i in range(n_blocks-1,-1,-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count//2, kernel=1))\n            cur_channels_count = cur_channels_count//2\n\n            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n            \n            self.denseBlocksUp.append(blk)\n            prev_block_channels = blk.get_out_ch()\n            cur_channels_count = prev_block_channels\n\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n               out_channels=n_classes, kernel_size=1, stride=1,\n               padding=0, bias=True)\n    \n    def v2_transform(self):        \n        for i in range( len(self.base)):\n            if isinstance(self.base[i], HarDBlock):\n                blk = self.base[i]\n                self.base[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=True)\n                self.base[i].transform(blk)\n            elif isinstance(self.base[i], nn.Sequential):\n                blk = self.base[i]\n                sz = blk[0].weight.shape\n                if sz[2] == 1:\n                    #self.base[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                    self.base[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                    self.base[i].bias[:] = blk[0].bias[:]\n\n        for i in range(self.n_blocks):\n            blk = self.denseBlocksUp[i]\n            self.denseBlocksUp[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers, list_out=False)\n            self.denseBlocksUp[i].transform(blk)\n  \n        for i in range(len(self.conv1x1_up)):\n            blk = self.conv1x1_up[i]\n            sz = blk[0].weight.shape\n            if sz[2] == 1:\n                #self.conv1x1_up[i] = CatConv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i] = nn.Conv2d(sz[1],sz[0],(1,1), relu=True)\n                self.conv1x1_up[i].weight[:,:,:,:] = blk[0].weight[:,:,:,:]\n                self.conv1x1_up[i].bias[:] = blk[0].bias[:]                 \n\n    def forward(self, x):\n        \n        skip_connections = []\n        size_in = x.size()\n        \n        \n        for i in range(len(self.base)):\n            x = self.base[i](x)\n            if i in self.shortcut_layers:\n                skip_connections.append(x)\n        out = x\n        \n        for i in range(self.n_blocks):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip, True)\n            out = self.conv1x1_up[i](out)\n            out = self.denseBlocksUp[i](out)\n        \n        out = self.finalConv(out)\n        \n        out = F.interpolate(\n                            out,\n                            size=(size_in[2], size_in[3]),\n                            mode=\"bilinear\",\n                            align_corners=True)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:04.067770Z","iopub.execute_input":"2022-05-04T19:55:04.068461Z","iopub.status.idle":"2022-05-04T19:55:04.247229Z","shell.execute_reply.started":"2022-05-04T19:55:04.068422Z","shell.execute_reply":"2022-05-04T19:55:04.246307Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class runningScore(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n        hist = np.bincount(\n            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n        ).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n\n    def get_scores(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return (\n            {\n                \"Overall Acc: \\t\": acc,\n                \"Mean Acc : \\t\": acc_cls,\n                \"FreqW Acc : \\t\": fwavacc,\n                \"Mean IoU : \\t\": mean_iu,\n            },\n            cls_iu,\n        )\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\n\nclass averageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:04.248743Z","iopub.execute_input":"2022-05-04T19:55:04.249004Z","iopub.status.idle":"2022-05-04T19:55:04.270012Z","shell.execute_reply.started":"2022-05-04T19:55:04.248971Z","shell.execute_reply":"2022-05-04T19:55:04.269253Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def cross_entropy2d(input, target, weight=None, size_average=True):\n    n, c, h, w = input.size()\n    nt, ht, wt = target.size()\n    #print(input.size(), target.size())\n    #nt, ht, wt = target.size()\n\n    if h != ht and w != wt:  # upsample labels\n        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n\n    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n    target = target.view(-1)\n    #print(input.size(), target.size())\n    #print(input[0, 0].dtype, target[0].dtype)\n    loss = F.cross_entropy(\n              input, target, weight=weight, size_average=size_average, ignore_index=11, reduction='mean')\n\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:04.271511Z","iopub.execute_input":"2022-05-04T19:55:04.271847Z","iopub.status.idle":"2022-05-04T19:55:04.282232Z","shell.execute_reply.started":"2022-05-04T19:55:04.271754Z","shell.execute_reply":"2022-05-04T19:55:04.281489Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n\n\n# Setup seeds\ntorch.manual_seed(1337)\ntorch.cuda.manual_seed(1337)\nnp.random.seed(1337)\nrandom.seed(1337)\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Setup Augmentations\n#augmentations = cfg[\"training\"].get(\"augmentations\", None)\n#data_aug = get_composed_augmentations(augmentations)\ndata_aug = None #Compose([RandomHorizontallyFlip(0.5), RandomCrop((270, 360))])\n\n# Setup Dataloader\n\n#data_loader = get_loader(cfg[\"data\"][\"dataset\"])\ndata_loader = camvidLoader\n#data_path = cfg[\"data\"][\"path\"]\ndata_path = '../input/camvid/CamVid/'\n\nt_loader = data_loader(\n    data_path,\n    is_transform=True,\n    #split=cfg[\"data\"][\"train_split\"],\n    split='train',\n    #img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n    img_size=(960, 720),\n    augmentations=data_aug,\n)\n\nv_loader = data_loader(\n    data_path,\n    is_transform=True,\n    #split=cfg[\"data\"][\"val_split\"],\n    split='val',\n    img_size=(960,720),\n)\n\nn_classes = t_loader.n_classes\ntrainloader = data.DataLoader(\n    t_loader,\n    #batch_size=cfg[\"training\"][\"batch_size\"],\n    batch_size=12,\n    #num_workers=cfg[\"training\"][\"n_workers\"],\n    shuffle=True,\n)\n\nvalloader = data.DataLoader(\n    v_loader, \n    #batch_size=cfg[\"training\"][\"batch_size\"],\n    batch_size=12, \n    #num_workers=cfg[\"training\"][\"n_workers\"]\n)\n\n# Setup Metrics\nrunning_metrics_val = runningScore(n_classes)\n\n# Setup Model\n\n#model = get_model(cfg[\"model\"], n_classes).to(device)\nmodel = hardnet()\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint( 'Parameters:',total_params )\n\nmodel = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\nmodel.apply(weights_init)\n#pretrained_path='weights/hardnet_petite_base.pth'\n#weights = torch.load(pretrained_path)\n#model.module.base.load_state_dict(weights)\n\n# Setup optimizer, lr_scheduler and loss function\n#optimizer_cls = get_optimizer(cfg)\noptimizer_cls = torch.optim.SGD\n#optimizer_params = {k: v for k, v in cfg[\"training\"][\"optimizer\"].items() if k != \"name\"}\n\noptimizer = optimizer_cls(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\nprint(\"Using optimizer {}\".format(optimizer))\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n\n#loss_fn = get_loss_function(cfg)\nloss_fn = cross_entropy2d\nprint(\"Using loss {}\".format(loss_fn))\n\nstart_iter = 0\nepoch = 0\nresume = None\nfile_checkpoint = 'hardnet_CamVid_checkpoint.pkl'\nif resume is not None:\n    if os.path.isfile(file_checkpoint):\n        print(\"Loading model and optimizer from checkpoint '{}'\".format(file_checkpoint))\n              \n        checkpoint = torch.load(file_checkpoint)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n        start_iter = checkpoint[\"iter\"]\n        epoch = checkpoint[\"epoch\"]\n\n    else:\n        print(\"No checkpoint found at '{}'\".format(file_checkpoint))\n\n# if cfg[\"training\"][\"finetune\"] is not None:\n#     if os.path.isfile(cfg[\"training\"][\"finetune\"]):\n#         logger.info(\n#             \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"finetune\"])\n#         )\n#         checkpoint = torch.load(cfg[\"training\"][\"finetune\"])\n#         model.load_state_dict(checkpoint[\"model_state\"])\n\nval_loss_meter = averageMeter()\ntime_meter = averageMeter()\n\nbest_iou = -100.0\nflag = True\nloss_all = 0\nloss_n = 0\nnum_epochs = 20\nper_epoch = 300\ntrain_iters = 6000\ni = start_iter\n#while i <= cfg[\"training\"][\"train_iters\"] and flag:\nwhile i <= train_iters and flag:\n    for (images, labels) in trainloader:\n        i += 1\n        start_ts = time.time()\n        \n        model.train()\n        images = images.to(device)\n        newlabels = torch.zeros((len(labels), 960, 720))\n        for j in range(len(labels)):\n          newlabels[j] = rgb_to_lbl(labels[j, :, :, :])\n        labels = newlabels\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        #print(images.size(), outputs.size(), labels.size())\n\n        #print(labels.size())\n        # print(labels.count_nonzero(), labels.size())\n        loss = loss_fn(input=outputs, target=labels.long())\n        loss.backward()\n        optimizer.step()\n        c_lr = scheduler.get_last_lr()\n\n        time_meter.update(time.time() - start_ts)\n        loss_all += loss.item()\n        loss_n += 1\n        \n        #if (i + 1) % cfg[\"training\"][\"print_interval\"] == 0:\n        \n        \n        if (i + 1) % 50 == 0:\n            fmt_str = \"Epoch {:d} Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}  lr={:.6f}\"\n            print_str = fmt_str.format(\n                epoch,\n                (i + 1) % per_epoch,\n                #cfg[\"training\"][\"train_iters\"],\n                per_epoch,\n                loss_all / loss_n,\n                #time_meter.avg / cfg[\"training\"][\"batch_size\"],\n                time_meter.avg / 12,\n                c_lr[0],\n            )\n            \n\n            print(print_str)\n            #logger.info(print_str)\n            #writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n            time_meter.reset()\n\n        # if (i + 1) % cfg[\"training\"][\"val_interval\"] == 0 or (i + 1) == cfg[\"training\"][\n        #     \"train_iters\"\n        # ]:\n        #print(i + 1)\n        if (i + 1) % per_epoch == 0:\n          epoch = epoch + 1\n        if (i + 1) % per_epoch == 0 or (i + 1) == train_iters:  \n          scheduler.step()\n          torch.cuda.empty_cache()\n          model.eval()\n          loss_all = 0\n          loss_n = 0\n          with torch.no_grad():\n              for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n                  images_val = images_val.to(device)\n                  newlabels = torch.zeros((len(labels_val), 960, 720))\n                  for j in range(len(labels_val)):\n                    newlabels[j] = rgb_to_lbl(labels_val[j, :, :, :])\n                  labels_val = newlabels\n                  labels_val = labels_val.to(device)\n\n                  outputs = model(images_val)\n                  val_loss = loss_fn(input=outputs, target=labels_val.long())\n\n                  pred = outputs.data.max(1)[1].cpu().numpy()\n                  gt = labels_val.data.cpu().numpy()\n\n                  running_metrics_val.update(gt, pred)\n                  val_loss_meter.update(val_loss.item())\n\n          #writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n          #logger.info(\"Iter %d Val Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n          print(format(\"Epoch %d Iter %d Val Loss: %.4f\" % (epoch, i + 1, val_loss_meter.avg)))\n\n          score, class_iou = running_metrics_val.get_scores()\n          for k, v in score.items():\n              print(k, v)\n              #logger.info(\"{}: {}\".format(k, v))\n              #writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n\n          #for k, v in class_iou.items():\n              #logger.info(\"{}: {}\".format(k, v))\n              #writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n\n          val_loss_meter.reset()\n          running_metrics_val.reset()\n          \n          state = {\n                \"epoch\": epoch,\n                \"iter\": i + 1,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scheduler_state\": scheduler.state_dict(),\n          }\n          save_path = os.path.join(\n              './',\n              \"{}_{}_checkpoint.pkl\".format('hardnet', 'CamVid'),\n          )\n          torch.save(state, save_path)\n\n#           if score[\"Mean IoU : \\t\"] >= best_iou:\n#               best_iou = score[\"Mean IoU : \\t\"]\n#               state = {\n#                   \"epoch\": i + 1,\n#                   \"model_state\": model.state_dict(),\n#                   \"best_iou\": best_iou,\n#               }\n              # save_path = os.path.join(\n              #     writer.file_writer.get_logdir(),\n              #     \"{}_{}_best_model.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n              # )\n              # torch.save(state, save_path)\n          torch.cuda.empty_cache()\n      \n          # if (i + 1) == cfg[\"training\"][\"train_iters\"]:\n        if (i + 1) == train_iters:\n            flag = False\n            break","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:55:04.283688Z","iopub.execute_input":"2022-05-04T19:55:04.283984Z","iopub.status.idle":"2022-05-05T05:45:50.722098Z","shell.execute_reply.started":"2022-05-04T19:55:04.283948Z","shell.execute_reply":"2022-05-05T05:45:50.721376Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Parameters: 4118865\nUsing optimizer SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.05\n    momentum: 0.9\n    nesterov: False\n    weight_decay: 0.0005\n)\nUsing loss <function cross_entropy2d at 0x7fb8d3cfde60>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n  warnings.warn(warning.format(ret))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 Iter [50/300]  Loss: 0.9164  Time/Image: 0.1121  lr=0.050000\nEpoch 0 Iter [100/300]  Loss: 0.6981  Time/Image: 0.1030  lr=0.050000\nEpoch 0 Iter [150/300]  Loss: 0.6088  Time/Image: 0.1027  lr=0.050000\nEpoch 0 Iter [200/300]  Loss: 0.5410  Time/Image: 0.1037  lr=0.050000\nEpoch 0 Iter [250/300]  Loss: 0.4888  Time/Image: 0.1032  lr=0.050000\nEpoch 0 Iter [0/300]  Loss: 0.4521  Time/Image: 0.1038  lr=0.050000\n","output_type":"stream"},{"name":"stderr","text":"9it [00:51,  5.68s/it]\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Iter 300 Val Loss: 0.3261\nOverall Acc: \t 0.7291790509259259\nMean Acc : \t 0.5241153275024899\nFreqW Acc : \t 0.5596061010139503\nMean IoU : \t 0.35290832403185457\nEpoch 1 Iter [50/300]  Loss: 0.2622  Time/Image: 0.1008  lr=0.045000\nEpoch 1 Iter [100/300]  Loss: 0.2407  Time/Image: 0.1020  lr=0.045000\nEpoch 1 Iter [150/300]  Loss: 0.2283  Time/Image: 0.1026  lr=0.045000\nEpoch 1 Iter [200/300]  Loss: 0.2165  Time/Image: 0.1014  lr=0.045000\nEpoch 1 Iter [250/300]  Loss: 0.2076  Time/Image: 0.1028  lr=0.045000\nEpoch 1 Iter [0/300]  Loss: 0.2033  Time/Image: 0.1021  lr=0.045000\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Iter 600 Val Loss: 0.2079\nOverall Acc: \t 0.7661574363425926\nMean Acc : \t 0.5660219060381598\nFreqW Acc : \t 0.6045616212775806\nMean IoU : \t 0.4081943871064802\nEpoch 2 Iter [50/300]  Loss: 0.1620  Time/Image: 0.1022  lr=0.040500\nEpoch 2 Iter [100/300]  Loss: 0.1533  Time/Image: 0.1016  lr=0.040500\nEpoch 2 Iter [150/300]  Loss: 0.1474  Time/Image: 0.1023  lr=0.040500\nEpoch 2 Iter [200/300]  Loss: 0.1417  Time/Image: 0.1019  lr=0.040500\nEpoch 2 Iter [250/300]  Loss: 0.1389  Time/Image: 0.1013  lr=0.040500\nEpoch 2 Iter [0/300]  Loss: 0.1356  Time/Image: 0.1020  lr=0.040500\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Iter 900 Val Loss: 0.1932\nOverall Acc: \t 0.7704432436342593\nMean Acc : \t 0.5994715444908038\nFreqW Acc : \t 0.6116788626684848\nMean IoU : \t 0.44690300935834537\nEpoch 3 Iter [50/300]  Loss: 0.1111  Time/Image: 0.1021  lr=0.036450\nEpoch 3 Iter [100/300]  Loss: 0.1094  Time/Image: 0.1029  lr=0.036450\nEpoch 3 Iter [150/300]  Loss: 0.1060  Time/Image: 0.1046  lr=0.036450\nEpoch 3 Iter [200/300]  Loss: 0.1039  Time/Image: 0.1040  lr=0.036450\nEpoch 3 Iter [250/300]  Loss: 0.1037  Time/Image: 0.1043  lr=0.036450\nEpoch 3 Iter [0/300]  Loss: 0.1036  Time/Image: 0.1022  lr=0.036450\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Iter 1200 Val Loss: 0.1955\nOverall Acc: \t 0.7706379629629629\nMean Acc : \t 0.6320759811466278\nFreqW Acc : \t 0.6171790310960431\nMean IoU : \t 0.4538477184990625\nEpoch 4 Iter [50/300]  Loss: 0.0874  Time/Image: 0.1028  lr=0.032805\nEpoch 4 Iter [100/300]  Loss: 0.0861  Time/Image: 0.1020  lr=0.032805\nEpoch 4 Iter [150/300]  Loss: 0.0842  Time/Image: 0.1023  lr=0.032805\nEpoch 4 Iter [200/300]  Loss: 0.0828  Time/Image: 0.1025  lr=0.032805\nEpoch 4 Iter [250/300]  Loss: 0.0814  Time/Image: 0.1028  lr=0.032805\nEpoch 4 Iter [0/300]  Loss: 0.0801  Time/Image: 0.1015  lr=0.032805\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Iter 1500 Val Loss: 0.1816\nOverall Acc: \t 0.7747244502314815\nMean Acc : \t 0.6314027783700402\nFreqW Acc : \t 0.6221422369999808\nMean IoU : \t 0.457529338683681\nEpoch 5 Iter [50/300]  Loss: 0.0731  Time/Image: 0.1011  lr=0.029525\nEpoch 5 Iter [100/300]  Loss: 0.0721  Time/Image: 0.1014  lr=0.029525\nEpoch 5 Iter [150/300]  Loss: 0.0706  Time/Image: 0.1017  lr=0.029525\nEpoch 5 Iter [200/300]  Loss: 0.0698  Time/Image: 0.1016  lr=0.029525\nEpoch 5 Iter [250/300]  Loss: 0.0688  Time/Image: 0.1038  lr=0.029525\nEpoch 5 Iter [0/300]  Loss: 0.0685  Time/Image: 0.1026  lr=0.029525\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Iter 1800 Val Loss: 0.1895\nOverall Acc: \t 0.7739232349537037\nMean Acc : \t 0.6409968443493165\nFreqW Acc : \t 0.6229546102106489\nMean IoU : \t 0.46663808132160617\nEpoch 6 Iter [50/300]  Loss: 0.0635  Time/Image: 0.1036  lr=0.026572\nEpoch 6 Iter [100/300]  Loss: 0.0647  Time/Image: 0.1016  lr=0.026572\nEpoch 6 Iter [150/300]  Loss: 0.0774  Time/Image: 0.1032  lr=0.026572\nEpoch 6 Iter [200/300]  Loss: 0.0757  Time/Image: 0.1034  lr=0.026572\nEpoch 6 Iter [250/300]  Loss: 0.0746  Time/Image: 0.1009  lr=0.026572\nEpoch 6 Iter [0/300]  Loss: 0.0738  Time/Image: 0.1020  lr=0.026572\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Iter 2100 Val Loss: 0.1822\nOverall Acc: \t 0.7764150173611111\nMean Acc : \t 0.6401669150288006\nFreqW Acc : \t 0.6238655330465417\nMean IoU : \t 0.4724287341255081\nEpoch 7 Iter [50/300]  Loss: 0.0585  Time/Image: 0.1021  lr=0.023915\nEpoch 7 Iter [100/300]  Loss: 0.0569  Time/Image: 0.1021  lr=0.023915\nEpoch 7 Iter [150/300]  Loss: 0.0557  Time/Image: 0.1024  lr=0.023915\nEpoch 7 Iter [200/300]  Loss: 0.0551  Time/Image: 0.1022  lr=0.023915\nEpoch 7 Iter [250/300]  Loss: 0.0546  Time/Image: 0.1027  lr=0.023915\nEpoch 7 Iter [0/300]  Loss: 0.0541  Time/Image: 0.1023  lr=0.023915\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Iter 2400 Val Loss: 0.1878\nOverall Acc: \t 0.7765721354166667\nMean Acc : \t 0.655632836093405\nFreqW Acc : \t 0.6263281652598406\nMean IoU : \t 0.4745234171288668\nEpoch 8 Iter [50/300]  Loss: 0.0488  Time/Image: 0.1022  lr=0.021523\nEpoch 8 Iter [100/300]  Loss: 0.0487  Time/Image: 0.1033  lr=0.021523\nEpoch 8 Iter [150/300]  Loss: 0.0484  Time/Image: 0.1032  lr=0.021523\nEpoch 8 Iter [200/300]  Loss: 0.0479  Time/Image: 0.1010  lr=0.021523\nEpoch 8 Iter [250/300]  Loss: 0.0472  Time/Image: 0.1021  lr=0.021523\nEpoch 8 Iter [0/300]  Loss: 0.0473  Time/Image: 0.1026  lr=0.021523\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Iter 2700 Val Loss: 0.1873\nOverall Acc: \t 0.7779149450231482\nMean Acc : \t 0.6595761790863939\nFreqW Acc : \t 0.6282103910621842\nMean IoU : \t 0.4783527192431466\nEpoch 9 Iter [50/300]  Loss: 0.0456  Time/Image: 0.1006  lr=0.019371\nEpoch 9 Iter [100/300]  Loss: 0.0442  Time/Image: 0.1016  lr=0.019371\nEpoch 9 Iter [150/300]  Loss: 0.0437  Time/Image: 0.1012  lr=0.019371\nEpoch 9 Iter [200/300]  Loss: 0.0434  Time/Image: 0.1018  lr=0.019371\nEpoch 9 Iter [250/300]  Loss: 0.0433  Time/Image: 0.1004  lr=0.019371\nEpoch 9 Iter [0/300]  Loss: 0.0427  Time/Image: 0.1016  lr=0.019371\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Iter 3000 Val Loss: 0.1917\nOverall Acc: \t 0.7777575810185186\nMean Acc : \t 0.6619475424622706\nFreqW Acc : \t 0.6282148855759331\nMean IoU : \t 0.47716531806695445\nEpoch 10 Iter [50/300]  Loss: 0.0401  Time/Image: 0.1020  lr=0.017434\nEpoch 10 Iter [100/300]  Loss: 0.0399  Time/Image: 0.1016  lr=0.017434\nEpoch 10 Iter [150/300]  Loss: 0.0396  Time/Image: 0.1018  lr=0.017434\nEpoch 10 Iter [200/300]  Loss: 0.0394  Time/Image: 0.1025  lr=0.017434\nEpoch 10 Iter [250/300]  Loss: 0.0390  Time/Image: 0.1017  lr=0.017434\nEpoch 10 Iter [0/300]  Loss: 0.0388  Time/Image: 0.1018  lr=0.017434\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Iter 3300 Val Loss: 0.1968\nOverall Acc: \t 0.776955859375\nMean Acc : \t 0.6594203273480131\nFreqW Acc : \t 0.6281784794188315\nMean IoU : \t 0.4741782536111181\nEpoch 11 Iter [50/300]  Loss: 0.0365  Time/Image: 0.1006  lr=0.015691\nEpoch 11 Iter [100/300]  Loss: 0.0365  Time/Image: 0.1016  lr=0.015691\nEpoch 11 Iter [150/300]  Loss: 0.0367  Time/Image: 0.1012  lr=0.015691\nEpoch 11 Iter [200/300]  Loss: 0.0367  Time/Image: 0.1012  lr=0.015691\nEpoch 11 Iter [250/300]  Loss: 0.0367  Time/Image: 0.1006  lr=0.015691\nEpoch 11 Iter [0/300]  Loss: 0.0365  Time/Image: 0.1013  lr=0.015691\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Iter 3600 Val Loss: 0.2014\nOverall Acc: \t 0.7764967447916666\nMean Acc : \t 0.6613153632628006\nFreqW Acc : \t 0.6273894154666565\nMean IoU : \t 0.47598459097844253\nEpoch 12 Iter [50/300]  Loss: 0.0355  Time/Image: 0.1021  lr=0.014121\nEpoch 12 Iter [100/300]  Loss: 0.0349  Time/Image: 0.1008  lr=0.014121\nEpoch 12 Iter [150/300]  Loss: 0.0347  Time/Image: 0.1015  lr=0.014121\nEpoch 12 Iter [200/300]  Loss: 0.0346  Time/Image: 0.1021  lr=0.014121\nEpoch 12 Iter [250/300]  Loss: 0.0343  Time/Image: 0.1018  lr=0.014121\nEpoch 12 Iter [0/300]  Loss: 0.0342  Time/Image: 0.1028  lr=0.014121\n","output_type":"stream"},{"name":"stderr","text":"9it [00:44,  4.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Iter 3900 Val Loss: 0.2066\nOverall Acc: \t 0.776597583912037\nMean Acc : \t 0.6608586546845059\nFreqW Acc : \t 0.6271128809648541\nMean IoU : \t 0.4774755938357416\nEpoch 13 Iter [50/300]  Loss: 0.0328  Time/Image: 0.1008  lr=0.012709\nEpoch 13 Iter [100/300]  Loss: 0.0332  Time/Image: 0.1005  lr=0.012709\nEpoch 13 Iter [150/300]  Loss: 0.0331  Time/Image: 0.1033  lr=0.012709\nEpoch 13 Iter [200/300]  Loss: 0.0327  Time/Image: 0.1009  lr=0.012709\nEpoch 13 Iter [250/300]  Loss: 0.0325  Time/Image: 0.1021  lr=0.012709\nEpoch 13 Iter [0/300]  Loss: 0.0323  Time/Image: 0.1012  lr=0.012709\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Iter 4200 Val Loss: 0.2021\nOverall Acc: \t 0.7777251302083333\nMean Acc : \t 0.6653081862544027\nFreqW Acc : \t 0.6293982405949509\nMean IoU : \t 0.4761429109123965\nEpoch 14 Iter [50/300]  Loss: 0.0316  Time/Image: 0.1006  lr=0.011438\nEpoch 14 Iter [100/300]  Loss: 0.0314  Time/Image: 0.1028  lr=0.011438\nEpoch 14 Iter [150/300]  Loss: 0.0313  Time/Image: 0.1012  lr=0.011438\nEpoch 14 Iter [200/300]  Loss: 0.0312  Time/Image: 0.1024  lr=0.011438\nEpoch 14 Iter [250/300]  Loss: 0.0311  Time/Image: 0.1013  lr=0.011438\nEpoch 14 Iter [0/300]  Loss: 0.0310  Time/Image: 0.1021  lr=0.011438\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Iter 4500 Val Loss: 0.2074\nOverall Acc: \t 0.776606785300926\nMean Acc : \t 0.6642421479685768\nFreqW Acc : \t 0.6279020571163425\nMean IoU : \t 0.47577754731463284\nEpoch 15 Iter [50/300]  Loss: 0.0295  Time/Image: 0.1015  lr=0.010295\nEpoch 15 Iter [100/300]  Loss: 0.0294  Time/Image: 0.1013  lr=0.010295\nEpoch 15 Iter [150/300]  Loss: 0.0296  Time/Image: 0.1014  lr=0.010295\nEpoch 15 Iter [200/300]  Loss: 0.0296  Time/Image: 0.1010  lr=0.010295\nEpoch 15 Iter [250/300]  Loss: 0.0298  Time/Image: 0.1013  lr=0.010295\nEpoch 15 Iter [0/300]  Loss: 0.0299  Time/Image: 0.1019  lr=0.010295\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 Iter 4800 Val Loss: 0.2090\nOverall Acc: \t 0.7761253327546296\nMean Acc : \t 0.6695508007796597\nFreqW Acc : \t 0.6289550733178491\nMean IoU : \t 0.47444823453377954\nEpoch 16 Iter [50/300]  Loss: 0.0363  Time/Image: 0.1010  lr=0.009265\nEpoch 16 Iter [100/300]  Loss: 0.0350  Time/Image: 0.1019  lr=0.009265\nEpoch 16 Iter [150/300]  Loss: 0.0334  Time/Image: 0.1034  lr=0.009265\nEpoch 16 Iter [200/300]  Loss: 0.0327  Time/Image: 0.1013  lr=0.009265\nEpoch 16 Iter [250/300]  Loss: 0.0320  Time/Image: 0.1006  lr=0.009265\nEpoch 16 Iter [0/300]  Loss: 0.0314  Time/Image: 0.1011  lr=0.009265\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 Iter 5100 Val Loss: 0.2094\nOverall Acc: \t 0.7774969618055556\nMean Acc : \t 0.6661705168738489\nFreqW Acc : \t 0.629251684862956\nMean IoU : \t 0.4776652178121465\nEpoch 17 Iter [50/300]  Loss: 0.0283  Time/Image: 0.1011  lr=0.008339\nEpoch 17 Iter [100/300]  Loss: 0.0281  Time/Image: 0.1019  lr=0.008339\nEpoch 17 Iter [150/300]  Loss: 0.0280  Time/Image: 0.1024  lr=0.008339\nEpoch 17 Iter [200/300]  Loss: 0.0280  Time/Image: 0.1018  lr=0.008339\nEpoch 17 Iter [250/300]  Loss: 0.0280  Time/Image: 0.1032  lr=0.008339\nEpoch 17 Iter [0/300]  Loss: 0.0279  Time/Image: 0.1022  lr=0.008339\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 Iter 5400 Val Loss: 0.2116\nOverall Acc: \t 0.7768587528935185\nMean Acc : \t 0.6696716271322071\nFreqW Acc : \t 0.6288264654430007\nMean IoU : \t 0.4757992930247621\nEpoch 18 Iter [50/300]  Loss: 0.0273  Time/Image: 0.1045  lr=0.007505\nEpoch 18 Iter [100/300]  Loss: 0.0271  Time/Image: 0.1036  lr=0.007505\nEpoch 18 Iter [150/300]  Loss: 0.0269  Time/Image: 0.1032  lr=0.007505\nEpoch 18 Iter [200/300]  Loss: 0.0269  Time/Image: 0.1026  lr=0.007505\nEpoch 18 Iter [250/300]  Loss: 0.0268  Time/Image: 0.1030  lr=0.007505\nEpoch 18 Iter [0/300]  Loss: 0.0268  Time/Image: 0.1045  lr=0.007505\n","output_type":"stream"},{"name":"stderr","text":"9it [00:45,  5.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 Iter 5700 Val Loss: 0.2124\nOverall Acc: \t 0.7774979166666667\nMean Acc : \t 0.6662098339579428\nFreqW Acc : \t 0.6304713116412349\nMean IoU : \t 0.47444165776623154\nEpoch 19 Iter [50/300]  Loss: 0.0260  Time/Image: 0.1049  lr=0.006754\nEpoch 19 Iter [100/300]  Loss: 0.0263  Time/Image: 0.1052  lr=0.006754\nEpoch 19 Iter [150/300]  Loss: 0.0264  Time/Image: 0.1053  lr=0.006754\nEpoch 19 Iter [200/300]  Loss: 0.0262  Time/Image: 0.1040  lr=0.006754\nEpoch 19 Iter [250/300]  Loss: 0.0262  Time/Image: 0.1041  lr=0.006754\nEpoch 19 Iter [0/300]  Loss: 0.0262  Time/Image: 0.1054  lr=0.006754\n","output_type":"stream"},{"name":"stderr","text":"9it [00:46,  5.11s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 20 Iter 6000 Val Loss: 0.2147\nOverall Acc: \t 0.776824609375\nMean Acc : \t 0.667184610220554\nFreqW Acc : \t 0.6287314666687848\nMean IoU : \t 0.4757722448298685\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}